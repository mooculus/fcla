\documentclass{ximera}

\input{../../preamble.tex}

\title{Linear Dependence and Spans}

\begin{document}
\begin{abstract}
  In a linearly dependent set, there is always a vector that can be written as a linear combination of the others. 
\end{abstract}
\maketitle

In any linearly dependent set there is always one vector that can be
written as a linear combination of the others.  This is the substance
of the upcoming \ref{theorem:DLDS}.  Perhaps this will explain the use
of the word ``dependent.''  In a linearly dependent set, at least one
vector ``depends'' on the others (via a linear combination).

Indeed, because \ref{theorem:DLDS} is an equivalence
(\ref{technique:E}) some authors use this condition as a definition
(\ref{technique:D}) of linear dependence.  Then linear independence is
defined as the logical opposite of linear dependence.  Of course, we
have \textit{chosen} to take \ref{definition:LICV} as our definition,
and then follow with \ref{theorem:DLDS} as a theorem.

If we use a linearly dependent set to construct a span, then we can
\textit{always} create the same infinite set with a starting set that
is one vector smaller in size.  However, this will not be possible if
we build a span from a linearly independent set.  So in a certain
sense, using a linearly independent set to formulate a span is the
best possible way---there are not any extra vectors being used to
build up all the necessary linear combinations.

\begin{theorem}[Dependency in Linearly Dependent Sets]
  \label{theorem:DLDS} Suppose that $S=\set{\vectorlist{u}{n}}$ is a
  set of vectors.  Then $S$ is a linearly dependent set if and only if
  there is an index $t$, $1\leq t\leq n$ such that $\vect{u_t}$ is a
  linear combination of the vectors
  $\vect{u}_1,\,\vect{u}_2,\,\vect{u}_3,\,\ldots,\,\vect{u}_{t-1},\,\vect{u}_{t+1},\,\ldots,\,\vect{u}_n$.

  \begin{proof}
    ($\Rightarrow$) Suppose that $S$ is linearly dependent, so there
    exists a nontrivial relation of linear dependence by
    \ref{definition:LICV}.  That is, there are scalars, $\alpha_i$,
    $1\leq i\leq n$, which are not all zero, such that
    \[
      \lincombo{\alpha}{u}{n}=\zerovector.
    \]
    Since the $\alpha_i$ cannot all be zero, choose one, say $\alpha_t$, that is nonzero.  Then,
    \begin{align*}
      \vect{u}_t
      &=\frac{-1}{\alpha_t}\left(-\alpha_t\vect{u}_t\right)&&\ref{property:MICN}\\
      &=
        \frac{-1}{\alpha_t}\left(
        \alpha_1\vect{u}_1+
        \cdots+
        \alpha_{t-1}\vect{u}_{t-1}+
        \alpha_{t+1}\vect{u}_{t+1}+
        \cdots+
        \alpha_n\vect{u}_n
        \right)&&\ref{theorem:VSPCV}\\
      &=
        \frac{-\alpha_1}{\alpha_t}\vect{u}_1+
        \cdots+
        \frac{-\alpha_{t-1}}{\alpha_t}\vect{u}_{t-1}+
        \frac{-\alpha_{t+1}}{\alpha_t}\vect{u}_{t+1}+
        \cdots+
        \frac{-\alpha_n}{\alpha_t}\vect{u}_n
                                                           &&\ref{theorem:VSPCV}
    \end{align*}
    Since the values of $\frac{\alpha_i}{\alpha_t}$ are again scalars,
    we have expressed $\vect{u}_t$ as a linear combination of the
    other elements of $S$.
    
    ($\Leftarrow$) Assume that the vector $\vect{u}_t$ is a linear
    combination of the other vectors in $S$.  Write this linear
    combination, denoting the relevant scalars as $\beta_1$, $\beta_2$,
    \ldots , $\beta_{t-1}$, $\beta_{t+1}$, \ldots , $\beta_n$, as
    \begin{align*}
      \vect{u_t}
      &=
        \beta_1\vect{u}_1+
        \beta_2\vect{u}_2+
        \cdots+
        \beta_{t-1}\vect{u}_{t-1}+
        \beta_{t+1}\vect{u}_{t+1}+
        \cdots+
        \beta_n\vect{u}_n
    \end{align*}
    
    Then we have
    \begin{align*}
      \beta_1\vect{u}_1
      &+\cdots+
        \beta_{t-1}\vect{u}_{t-1}+
        (-1)\vect{u}_t+
        \beta_{t+1}\vect{u}_{t+1}+
        \cdots+
        \beta_n\vect{u}_n\\
      &=\vect{u}_t+(-1)\vect{u}_t&&\ref{theorem:VSPCV}\\
      &=\left(1+\left(-1\right)\right)\vect{u}_t&&\ref{property:DSAC}\\
      &=0\vect{u}_t&&\ref{property:AICN}\\
      &=\zerovector&&\ref{definition:CVSM}
    \end{align*}
    
    So the scalars
    $\beta_1,\,\beta_2,\,\beta_3,\,\ldots,\,\beta_{t-1},\,\beta_t=-1,\beta_{t+1},\,\,\ldots,\,\beta_n$
    provide a
    \wordChoice{\choice[correct]{nontrivial}\choice{trivial}} linear
    combination of the vectors in $S$, thus establishing that $S$ is a
    linearly \wordChoice{\choice{independent}\choice[correct]{dependent}} set.
    
  \end{proof}
\end{theorem}

This theorem can be used, sometimes repeatedly, to whittle down the
size of a set of vectors used in a span construction.  We have seen
some of this already, but in the next example we will detail some of
the subtleties.

\begin{example}[Reducing a span in $\real{5}$]
  Consider the set of $n=4$ vectors from $\real{5}$,
  \[
    R=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}
    =
    \set{
      \colvector{1\\2\\-1\\3\\2},\,
      \colvector{2\\1\\3\\1\\2},\,
      \colvector{0\\-7\\6\\-11\\-2},\,
      \colvector{4\\1\\2\\1\\6}
    }
  \]
  and define $V=\spn{R}$.
  
  To employ \ref{theorem:LIVHS}, we form a $5\times 4$ matrix, $D$, and
  row-reduce to understand solutions to the homogeneous system
  $\homosystem{D}$,
  \[
    D=
    \begin{bmatrix}
      1&2&0&4\\
      2&1&-7&1\\
      -1&3&6&2\\
      3&1&-11&1\\
      2&2&-2&6
    \end{bmatrix}
    \rref
    \begin{bmatrix}
      \leading{1}&0&0&4\\
      0&\leading{1}&0&0\\
      0&0&\leading{1}&1\\
      0&0&0&0\\
      0&0&0&0
    \end{bmatrix}
  \]

  We can find infinitely many solutions to this system, most of them
  nontrivial, and we choose any one we like to build a relation of
  linear dependence on $R$.  Let us begin with $x_4=1$, to find the
  solution
  \[
    \colvector{-4\\0\\-1\\\answer{1}}
  \]

  So we can write the relation of linear dependence,
  \[
    (-4)\vect{v}_1+0\vect{v}_2+(-1)\vect{v}_3+\answer{1}\vect{v}_4=\zerovector
  \]

  \ref{theorem:DLDS} guarantees that we can solve this relation of
  linear dependence for \textit{some} vector in $R$, but the choice of
  which one is up to us.  Notice however that $\vect{v}_2$ has a zero
  coefficient.  In this case, we cannot choose to solve for
  $\vect{v}_2$.  Maybe some other relation of linear dependence would
  produce a nonzero coefficient for $\vect{v}_2$ if we just had to
  solve for this vector.  Unfortunately, this example has been
  engineered to \textit{always} produce a zero coefficient here, as
  you can see from solving the homogeneous system.  Every solution has
  $x_2=0$!

  OK, if we are convinced that we cannot solve for $\vect{v}_2$, let
  us instead solve for $\vect{v}_3$,
  \[
    \vect{v}_3=(-4)\vect{v}_1+0\vect{v}_2+1\vect{v}_4=(-4)\vect{v}_1+1\vect{v}_4
  \]
  
  We now claim that this particular equation will allow us to write
  \[
    V=\spn{R}=
    \spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}}=
    \spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_4}}
  \]
  in essence declaring $\vect{v}_3$ as surplus for the task of
  building $V$ as a span.  This claim is an equality of two sets, so
  we will use \ref{definition:SE} to establish it carefully.  Let
  $R^\prime=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_4}$ and
  $V^\prime=\spn{R^\prime}$.  We want to show that $V=V^\prime$.

  First show that $V^\prime\subseteq V$.  Since every vector of
  $R^\prime$ is in $R$, any vector we can construct in $V^\prime$ as a
  linear combination of vectors from $R^\prime$ can also be
  constructed as a vector in $V$ by the same linear combination of the
  same vectors in $R$.  That was easy, now turn it around.
  
  Next show that $V\subseteq V^\prime$.  Choose any $\vect{v}$ from $V$.  So there are scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4$ such that
  \begin{align*}
    \vect{v}&=
              \alpha_1\vect{v}_1+\alpha_2\vect{v}_2+\alpha_3\vect{v}_3+\alpha_4\vect{v}_4\\
            &=\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+
              \alpha_3\left((-4)\vect{v}_1+1\vect{v}_4\right)+
              \alpha_4\vect{v}_4\\
            &=\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+
              \left((-4\alpha_3)\vect{v}_1+\alpha_3\vect{v}_4\right)+
              \alpha_4\vect{v}_4\\
            &=\left(\alpha_1-4\alpha_3\right)\vect{v}_1+
              \alpha_2\vect{v}_2+
              \left(\alpha_3+\alpha_4\right)\vect{v}_4.
  \end{align*}
  
  This equation says that $\vect{v}$ can then be written as a linear
  combination of the vectors in $R^\prime$ and hence qualifies for
  membership in $V^\prime$.  So $V\subseteq V^\prime$ and we have
  established that $V=V^\prime$.

  If $R^\prime$ was also linearly dependent (it is not), we could
  reduce the set even further.  Notice that we could have chosen to
  eliminate any one of $\vect{v}_1$, $\vect{v}_3$ or $\vect{v}_4$, but
  somehow $\vect{v}_2$ is essential to the creation of $V$ since it
  cannot be replaced by any linear combination of $\vect{v}_1$,
  $\vect{v}_3$ or $\vect{v}_4$.

\end{example}


\end{document}