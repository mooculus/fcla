\documentclass{ximera}

\input{../../preamble.tex}

\title{Orthogonal Vectors}

\begin{document}
\begin{abstract}
``Orthogonal'' is a generalization of ``perpendicular.''
\end{abstract}
\maketitle

``Orthogonal'' is a generalization of ``perpendicular.''  You may have
used mutually perpendicular vectors in a physics class, or you may
recall from a calculus class that perpendicular vectors have a zero
dot product.  We will now extend these ideas into the realm of higher
dimensions and complex scalars.

\begin{definition}[Orthogonal Vectors]
  A pair of vectors, $\vect{u}$ and $\vect{v}$, from $\complex{m}$ are
  \dfn{orthogonal} if their inner product is zero, that is,
  $\innerproduct{\vect{u}}{\vect{v}}=0$.
\end{definition}

\begin{example}[Orthogonal vectors]
  Consider the vectors
  \begin{align*}
    \vect{u}&=\colvector{2 + 3i\\4 - 2i\\1 + i\\1 + i} \\
    \vect{a}&=\colvector{1 + i\\2 + 3i\\4 - 6i\\1} \\
    \vect{b}&=\colvector{1 - i\\2 + 3i\\4 - 6i\\1}.
  \end{align*}
  Then
  \begin{multipleChoice}
    \choice{$\vect{u}$ and $\vect{a}$ are orthogonal.}
    \choice[correct]{$\vect{u}$ and $\vect{b}$ are orthogonal.}
  \end{multipleChoice}
  
  \begin{hint}
    \begin{align*}
      \innerproduct{\vect{u}}{\vect{v}}
      &=(2-3i)(1-i)+(4+2i)(2+3i)+(1-i)(4-6i)+(1-i)(1)\\
      &=(-1-5i)+(2+16i)+(-2-10i)+(1-i)\\
      &=0+0i.
    \end{align*}
  \end{hint}
\end{example}

We extend this definition to whole sets by requiring vectors to be
pairwise orthogonal.  Despite using the same word, careful thought
about what objects you are using will eliminate any source of
confusion.

\begin{definition}[Orthogonal Set of Vectors]

  Suppose that $S=\set{\vectorlist{u}{n}}$ is a set of vectors from
  $\complex{m}$.  Then $S$ is an \dfn{orthogonal set} if every pair of
  different vectors from $S$ is orthogonal, that is
  $\innerproduct{\vect{u}_i}{\vect{u}_j}=0$ whenever $i\neq j$.

\end{definition}

We now define the prototypical orthogonal set, which we will reference
repeatedly.

\begin{definition}[Standard Unit Vectors]

  Let $\vect{e}_j\in\complex{m}$, $1\leq j\leq m$ denote the column
  vectors defined by
  \begin{align*}
    \vectorentry{\vect{e}_j}{i}
    &=
      \begin{cases}
        0&\text{if $i\neq j$}\\
        1&\text{if $i=j$}
      \end{cases}
  \end{align*}

  Then the set
  \begin{align*}
    \set{\vectorlist{e}{m}}&=\setparts{\vect{e}_j}{1\leq j\leq m}
  \end{align*}
  is the set of \dfn{standard unit vectors} in $\complex{m}$.
  
\end{definition}

Notice that $\vect{e}_j$ is identical to column $j$ of the $m\times m$
identity matrix $I_m$, and is a pivot column for $I_m$, since the
identity matrix is in reduced row-echelon form.  These observations
will often be useful.  We will reserve the notation $\vect{e}_i$ for
these vectors.  It is not hard to see that the set of standard unit
vectors is an orthogonal set.

\begin{example}[Standard Unit Vectors are an Orthogonal Set]

Compute the inner product of two distinct vectors from the set of standard unit vectors (\ref{definition:SUV}), say $\vect{e}_i$, $\vect{e}_j$, where $i\neq j$, to discover that in this case
\[
\innerproduct{\vect{e}_i}{\vect{e}_j}&=\answer{0}
\]
\begin{hint}
\begin{align*}
  \innerproduct{\vect{e}_i}{\vect{e}_j}&=
                                         \conjugate{0}0+
                                         \conjugate{0}0+\cdots+
                                         \conjugate{1}0+\cdots+
                                         \conjugate{0}0+\cdots+
                                         \conjugate{0}1+\cdots+
                                         \conjugate{0}0+
                                         \conjugate{0}0\\
                                       &=0(0)+0(0)+\cdots+1(0)+\cdots+0(1)+\cdots+0(0)+0(0)\\
                                       &=0
\end{align*}
\end{hint}

So the set $\set{\vectorlist{e}{m}}$ is an orthogonal set.
\end{example}

\begin{example}[An orthogonal set]

The set
\[
\set{\vect{x}_1,\,\vect{x}_2,\,\vect{x}_3,\,\vect{x}_4}=
\set{
\colvector{1+i\\1\\1-i\\i},\,
\colvector{1+5i\\6+5i\\-7-i\\1-6i},\,
\colvector{-7+34i\\-8-23i\\-10+22i\\30+13i},\,
\colvector{-2-4i\\6+i\\4+3i\\6-i}
}
\]
is an orthogonal set.


Since the inner product is anti-commutative (\ref{theorem:IPAC}) we can test pairs of different vectors in any order.  If the result is zero, then it will also be zero if the inner product is computed in the opposite order.  This means there are six different pairs of vectors to use in an inner product computation.
\begin{align*}
\innerproduct{\vect{x}_1}{\vect{x}_3}&=
(1-i)(-7+34i)+(1)(-8-23i)+(1+i)(-10+22i)+(-i)(30+13i)\\
&=(\answer{27+41i})+(-8-23i)+(-32+12i)+(13-30i)\\
&=0+0i
<intertext>and</intertext>
\innerproduct{\vect{x}_2}{\vect{x}_4}&=
(1-5i)(-2-4i)+(6-5i)(6+i)+(-7+i)(4+3i)+(1+6i)(6-i)\\
&=(-22+6i)+(41-24i)+(-31-17i)+(12+35i)\\
&=0+0i
\end{align*}
You can practice your inner products on the other four.

\end{example}

So far, this section has seen lots of definitions, and lots of
theorems establishing un-surprising consequences of those definitions.
But here is our first theorem that suggests that inner products and
orthogonal vectors have some utility.  It is also one of our first
illustrations of how to arrive at linear independence as the
conclusion of a theorem.

\begin{theorem}[Orthogonal Sets are Linearly Independent]
\label{theorem:OSLI}

Suppose that $S$ is an orthogonal set of nonzero vectors.  Then $S$ is linearly independent.

\begin{proof}
  Let $S=\set{\vectorlist{u}{n}}$ be an orthogonal set of nonzero
  vectors.  To prove the linear independence of $S$, we can appeal to
  the definition (\ref{definition:LICV}) and begin with an arbitrary
  relation of linear dependence (\ref{definition:RLDCV}),
  \[
    \lincombo{\alpha}{u}{n}=\zerovector.
  \]
  
  Then, for every $1\leq i\leq n$, we have
  \begin{align*}
    &\alpha_i\innerproduct{\vect{u}_i}{\vect{u}_i}\\
    &\quad\quad=\alpha_1(0)+\alpha_2(0)+\cdots+\alpha_i\innerproduct{\vect{u}_i}{\vect{u}_i}+\cdots+\alpha_n(0)
    \\ %&&\ref{property:ZCN}\\
    &\quad\quad=
      \alpha_1\innerproduct{\vect{u}_i}{\vect{u}_1}+
      \cdots+
      \alpha_i\innerproduct{\vect{u}_i}{\vect{u}_i}+
      \cdots+
      \alpha_n\innerproduct{\vect{u}_i}{\vect{u}_n}
    \\ %&&\ref{definition:OSV}\\
    &\quad\quad=
      \innerproduct{\vect{u}_i}{\alpha_1\vect{u}_1}+
      \innerproduct{\vect{u}_i}{\alpha_2\vect{u}_2}+
      \cdots+
      \innerproduct{\vect{u}_i}{\alpha_n\vect{u}_n}
    \\ %&&\ref{theorem:IPSM}\\
    &\quad\quad=
      \innerproduct{\vect{u}_i}{\lincombo{\alpha}{u}{n}}
    \\ %&&\ref{theorem:IPVA}\\
    &\quad\quad=
      \innerproduct{\vect{u}_i}{\zerovector}
    \\ %&&\ref{definition:RLDCV}\\
    &\quad\quad=0
    \\ %&&\ref{definition:IP}
  \end{align*}
  
  Because $\vect{u}_i$ was assumed to be nonzero, \ref{theorem:PIP}
  says $\innerproduct{\vect{u}_i}{\vect{u}_i}$ is nonzero and thus
  $\alpha_i$ must be zero.  So we conclude that $\alpha_i=0$ for all
  $1\leq i\leq n$ in any relation of linear dependence on $S$.  But
  this says that $S$ is a linearly independent set since the only way
  to form a relation of linear dependence is the trivial way
  (\ref{definition:LICV}).
\end{proof}
\end{theorem}

\end{document}
