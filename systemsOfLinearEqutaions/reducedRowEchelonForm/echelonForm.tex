\documentclass{ximera}

\input{../../preamble.tex}

\title{Reduced Row-Echelon Form}

\begin{document}
\begin{abstract}
  What is this ``simpler'' form for a matrix, and just how do we get it?
\end{abstract}
\maketitle

The preceding example amply illustrates the definitions and theorems
we have seen so far.  But it still leaves two questions unanswered.
Exactly what is this ``simpler'' form for a matrix, and just how do we
get it?  Here is the answer to the first question, a definition of
reduced row-echelon form.

\begin{definition}[Reduced Row-Echelon Form]
A matrix is in \dfn{reduced row-echelon form} if it meets all of the following conditions:
\begin{enumerate}
\item If there is a row where every entry is zero, then this row lies below any other row that contains a nonzero entry.
\item The leftmost nonzero entry of a row is equal to 1.
\item The leftmost nonzero entry of a row is the only nonzero entry in its column.
\item Consider any two different leftmost nonzero entries, one located in row $i$, column $j$ and the other located in row $s$, column $t$.  If $s>i$, then $t>j$.
\end{enumerate}
A row of only zero entries is called a \dfn{zero row} and the leftmost nonzero entry of a nonzero row is a \dfn{leading 1}.  A column containing a leading 1 will be called a \dfn{pivot column}.  The number of nonzero rows will be denoted by $r$, which is also equal to the number of leading 1's and the number of pivot columns.
\end{definition}

The set of column indices for the pivot columns will be denoted by $D=\set{d_1,\,d_2,\,d_3,\,\ldots,\,d_r}$ where
$d_1<d_2<d_3<\cdots<d_r$,
while the columns that are not pivot columns will be denoted as $F=\set{f_1,\,f_2,\,f_3,\,\ldots,\,f_{n-r}}$ where
$f_1<f_2<f_3<\cdots<f_{n-r}$.

The principal feature of reduced row-echelon form is the pattern of
leading 1's guaranteed by conditions (2) and (4), reminiscent of a
flight of geese, or steps in a staircase, or water cascading down a
mountain stream.

There are a number of new terms and notation introduced in this
definition, which should make you suspect that this is an important
definition.  Given all there is to digest here, we will mostly save
the use of $D$ and $F$ until later.
However, one important point to make here is that all of these terms
and notation apply to a matrix.  Sometimes we will employ these terms
and sets for an augmented matrix, and other times it might be a
coefficient matrix.  So always give some thought to exactly which type
of matrix you are analyzing.

\begin{example}
  Consider the matrix $C$.
  \[
    C=
    \begin{bmatrix}
      1&-3&0&6&0&0&-5&9\\
      0&0&0&0&1&0&3&-7\\
      0&0&0&0&0&1&7&3\\
      0&0&0&0&0&0&0&0\\
      0&0&0&0&0&0&0&0
    \end{bmatrix}.
  \]
  Is it in reduced row-echelon form?
  \begin{multipleChoice}
    \choice[correct]{Yes.}
    \choice{No.}
  \end{multipleChoice}
  
  \begin{feedback}
    It \textbf{is} in row-echelon form.  This matrix has two zero rows and three pivot columns.
  \end{feedback}
  
  So $r=\answer{3}$.  Columns 1, 5, and \answer{6} are the three pivot columns, so $D=\set{1,\,5,\,6}$ and then $F=\set{2,\,3,\,4,\,7,\,8}$.
\end{example}

\begin{example}
  Consider the matrix $E$.
  \[
    E=
    \begin{bmatrix}
      1&0&-3&0&6&0&7&-5&9\\
      0&0&0&5&0&1&0&3&-7\\
      0&0&0&0&0&0&0&0&0\\
      0&1&0&0&0&0&0&-4&2\\
      0&0&0&0&0&0&1&7&3\\
      0&0&0&0&0&0&0&0&0
    \end{bmatrix}.
  \]
  Is it in reduced row-echelon form?
  \begin{multipleChoice}
    \choice{Yes.}
    \choice[correct]{No.}
  \end{multipleChoice}
\end{example}

\begin{theorem}[Row-Equivalent Matrix in Echelon Form]
  Suppose $A$ is a matrix.  Then there is a matrix $B$ so that
  \begin{enumerate}
  \item $A$ and $B$ are row-equivalent.
  \item $B$ is in reduced row-echelon form.
  \end{enumerate}

\begin{proof}
Suppose that $A$ has $m$ rows and $n$ columns.  We will describe a process for converting $A$ into $B$ via row operations.  This procedure is known as \dfn{Gauss-Jordan elimination}.  Tracing through this procedure will be easier if you recognize that $i$ refers to a row that is being converted, $j$ refers to a column that is being converted, and $r$ keeps track of the number of nonzero rows.  Here we go.

Begin by setting $j=0$ and $r=0$.  Then
\begin{enumerate}
\item Increase $j$ by 1.  If $j$ now equals $n+1$, then stop.
\item Examine the entries of $A$ in column $j$ located in rows $r+1$ through $m$.  If all of these entries are zero, then go to Step~(a).
\item Choose a row from rows $r+1$ through $m$ with a nonzero entry in column $j$.  Let $i$ denote the index for this row.
\item Increase $r$ by 1.
\item Use the first row operation to swap rows $i$ and $r$.
\item Use the second row operation to convert the entry in row $r$ and column $j$ to a 1.
\item Use the third row operation with row $r$ to convert every other entry of column $j$ to zero.
\item Go to Step~(a).
\end{enumerate}

The result of this procedure is that the matrix $A$ is converted to a
matrix in reduced row-echelon form, which we will refer to as $B$.  We
need to now prove this claim by showing that the converted matrix is
in row-echelon form.  First, the matrix is only converted through row
operations, so $A$ and $B$ are row-equivalent.

It is a bit more work to be certain that $B$ is in reduced row-echelon
form.  We claim that as we begin Step~(a), the first $j$ columns of the
matrix are in reduced row-echelon form with $r$ nonzero rows.
Certainly this is true at the start when $j=\answer{0}$, since the
matrix has no columns and so vacuously meets the conditions of being
in row-reduced echelon form with $r=0$ nonzero rows.

In Step~(a) we increase $j$ by 1 and begin to work with the next column.
There are two possible outcomes for Step~(b).  Suppose that every entry
of column $j$ in rows $r+1$ through $m$ is zero.  Then with no changes
we recognize that the first $j$ columns of the matrix has its first
$r$ rows still in reduced-row echelon form, with the final $m-r$ rows
still all zero.

Suppose instead that the entry in row $i$ of column $j$ is nonzero.
Notice that since $r+1\leq i\leq m$, we know the first $j-1$ entries
of this row are all zero.  Now, in Step~(d) we increase $r$ by 1, and
then embark on building a new nonzero row.  In Step~(e) we swap row
$r$ and row $i$.  In the first $j$ columns, the first $r-1$ rows
remain in reduced row-echelon form after the swap.  In Step~(f) we
multiply row $r$ by a nonzero scalar, creating a 1 in the entry in
column $\answer{j}$ of row $\answer{i}$, and not changing any other
rows.  This new leading 1 is the first nonzero entry in its row, and
is located to the right of all the leading 1's in the preceding $r-1$
rows.  With Step~(g) we insure that every entry in the column with
this new leading 1 is now zero, as required for reduced row-echelon
form.  Also, rows $r+1$ through $m$ are now all zeros in the first $j$
columns, so we now only have one new nonzero row, consistent with our
increase of $r$ by one.  Furthermore, since the first $j-1$ entries of
row $r$ are zero, the employment of the third row operation does not
destroy any of the necessary features of rows $1$ through $r-1$ and
rows $r+1$ through $m$, in columns $1$ through $j-1$.

So at this stage, the first $j$ columns of the matrix are in reduced
row-echelon form.  When Step~(a) finally increases $j$ to $\answer{n+1}$, then
the procedure is completed and the full $n$ columns of the matrix are
in reduced row-echelon form, with the value of $r$ correctly recording
the number of nonzero rows.
\end{proof}
\end{theorem}

So now we can put it all together.  Begin with a system of linear
equations, and represent the system by its augmented matrix.  Use row
operations to convert this matrix into reduced row-echelon form, using
the procedure outlined in the proof.  We can always accomplish this,
and that the result is row-equivalent to the original augmented
matrix.  Since the matrix in reduced-row echelon form has the same
solution set, we can analyze the row-reduced version instead of the
original matrix, viewing it as the augmented matrix of a different
system of equations.  The beauty of augmented matrices in reduced
row-echelon form is that the solution sets to the systems they
represent can be easily determined.

We will see through the course that almost every interesting property
of a matrix can be discerned by looking at a row-equivalent matrix in
reduced row-echelon form.  For this reason it is important to know
that the matrix $B$ is guaranteed to exist by the theorem is also
unique.

\begin{theorem}[Reduced Row-Echelon Form is Unique]
Suppose that $A$ is an $m\times n$ matrix and that $B$ and $C$ are $m\times n$ matrices that are row-equivalent to $A$ and in reduced row-echelon form.  Then $B=C$.

\begin{proof}
We need to begin with no assumptions about any relationships between $B$ and $C$, other than they are both in reduced row-echelon form, and they are both row-equivalent to $A$.

If $B$ and $C$ are both row-equivalent to $A$, then they are row-equivalent to each other.  Repeated row operations on a matrix combine the rows with each other using operations that are linear, and are identical in each column.  A key observation for this proof is that each individual row of $B$ is linearly related to the rows of $C$.  This relationship is different for each row of $B$, but once we fix a row, the relationship is the same across columns.  More precisely, there are scalars $\delta_{ik}$, $1\leq i,k\leq m$ such that for any $1\leq i\leq m$, $1\leq j\leq n$,
\begin{align*}
\matrixentry{B}{ij}
&=\sum_{k=1}^{m}\delta_{ik}\matrixentry{C}{kj}
\end{align*}
You should read this as saying that an entry of row $i$ of $B$ (in column $j$) is a linear function of the entries of all the rows of $C$ that are also in column $j$, and the scalars ($\delta_{ik}$) depend on which row of $B$ we are considering (the $i$ subscript on $\delta_{ik}$), but are the same for every column (no dependence on $j$ in $\delta_{ik}$).  This idea may be complicated now, but will feel more familiar once we discuss ``linear combinations'' and moreso when we discuss ``row spaces.'

We now repeatedly exploit the fact that $B$ and $C$ are in reduced row-echelon form.  Recall that a pivot column is all zeros, except a single one.  More carefully, if $R$ is a matrix in reduced row-echelon form, and $d_\ell$ is the index of a pivot column, then $\matrixentry{R}{kd_\ell}=1$ precisely when $k=\ell$ and is otherwise zero.  Notice also that any entry of $R$ that is both below the entry in row $\ell$ and to the left of column $d_\ell$ is also zero (with below and left understood to include equality).  In other words, look at examples of matrices in reduced row-echelon form and choose a leading 1 (with a box around it).  The rest of the column is also zeros, and the lower left ``quadrant'' of the matrix that begins here is totally zeros.

Assuming no relationship about the form of $B$ and $C$, let $B$ have $r$ nonzero rows and denote the pivot columns as $D=\set{\scalarlist{d}{r}}$.  For $C$ let $r^\prime$ denote the number of nonzero rows and denote the pivot columns $D^\prime=\set{\,\scalarlist{d^\prime}{r^\prime}}$.  There are four steps in the proof, and the first three are about showing that $B$ and $C$ have the same number of pivot columns, in the same places.  In other words, the ``primed'' symbols are a necessary fiction.

\paragraph*{First Step.}  Suppose that $d_1 < d^\prime_1$.  Then
\begin{align*}
1
&=\matrixentry{B}{1d_1}
\\
&=\sum_{k=1}^{m}\delta_{1k}\matrixentry{C}{kd_1}\\
&=\sum_{k=1}^{m}\delta_{1k}(0)
&&d_1<d^\prime_1\\
&=0
\end{align*}
The entries of $C$ are all zero since they are left and below of the leading 1 in row 1 and column $d^\prime_1$ of $C$.  This is a contradiction, so we know that $d_1\geq d^\prime_1$.  By an entirely similar argument, reversing the roles of $B$ and $C$, we could conclude that $d_1\leq d^\prime_1$.  Together this means that $d_1=d^\prime_1$.</p>

\paragraph*{Second Step.}  Suppose that we have determined that $d_1=d^\prime_1$, $d_2=d^\prime_2$, $d_3=d^\prime_3$, <ellipsis /> $d_p=d^\prime_p$.  Let us now show that $d_{p+1}=d^\prime_{p+1}$.  Working towards a contradiction, suppose that $d_{p+1}<d^\prime_{p+1}$.  For $1\leq\ell\leq p$,
\begin{align*}
0
&=\matrixentry{B}{p+1,d_\ell}
\\
&=\sum_{k=1}^{m}\delta_{p+1,k}\matrixentry{C}{kd_\ell}\\
&=\sum_{k=1}^{m}\delta_{p+1,k}\matrixentry{C}{kd^\prime_\ell}\\
&=
\delta_{p+1,\ell}\matrixentry{C}{\ell d^\prime_\ell}+
\sum_{\substack{k=1\\k\neq\ell}}^{m}\delta_{p+1,k}\matrixentry{C}{kd^\prime_\ell}
\\
&=
\delta_{p+1,\ell}(1)+
\sum_{\substack{k=1\\k\neq\ell}}^{m}\delta_{p+1,k}(0)
\\
&=\delta_{p+1,\ell}
\end{align*}
Now,
\begin{align*}
1
&=\matrixentry{B}{p+1,d_{p+1}}\\
&=\sum_{k=1}^{m}\delta_{p+1,k}\matrixentry{C}{kd_{p+1}}\\
&=
\sum_{k=1}^{p}\delta_{p+1,k}\matrixentry{C}{kd_{p+1}}+
\sum_{k=p+1}^{m}\delta_{p+1,k}\matrixentry{C}{kd_{p+1}}\\
&=
\sum_{k=1}^{p}(0)\matrixentry{C}{kd_{p+1}}+
\sum_{k=p+1}^{m}\delta_{p+1,k}\matrixentry{C}{kd_{p+1}}\\
&=\sum_{k=p+1}^{m}\delta_{p+1,k}\matrixentry{C}{kd_{p+1}}\\
&=\sum_{k=p+1}^{m}\delta_{p+1,k}(0)
&&d_{p+1}<d^\prime_{p+1}\\
&=0
\end{align*}
This contradiction shows that
$d_{p+1}\geq d^\prime_{p+1}$.  By an entirely similar argument, we could conclude that $d_{p+1}\leq d^\prime_{p+1}$, and therefore $d_{p+1}=d^\prime_{p+1}$.

\paragraph*{Third Step.}  Now we establish that $r=r^\prime$.  Suppose that $r^\prime<r$.  By the arguments above, we know that $d_1=d^\prime_1$, $d_2=d^\prime_2$, $d_3=d^\prime_3$, \ldots, $d_{r^\prime}=d^\prime_{r^\prime}$.   For $1\leq\ell\leq r^\prime<r$,
\begin{align*}
0
&=\matrixentry{B}{rd_\ell}
\\
&=\sum_{k=1}^{m}\delta_{rk}\matrixentry{C}{kd_\ell}\\
&=
\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kd_\ell}
+
\sum_{k=r^\prime+1}^{m}\delta_{rk}\matrixentry{C}{kd_\ell}
\\
&=
\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kd_\ell}
+
\sum_{k=r^\prime+1}^{m}\delta_{rk}(0)
\\
&=\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kd_\ell}\\
&=\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kd^\prime_\ell}\\
&=
\delta_{r\ell}\matrixentry{C}{\ell d^\prime_\ell}
+
\sum_{\substack{k=1\\k\neq\ell}}^{r^\prime}\delta_{rk}\matrixentry{C}{kd^\prime_\ell}
\\
&=
\delta_{r\ell}(1)
+
\sum_{\substack{k=1\\k\neq\ell}}^{r^\prime}\delta_{rk}(0)
\\
&=\delta_{r\ell}
\end{align*}
Now examine the entries of row $r$ of $B$,
\begin{align*}
\matrixentry{B}{rj}
&=\sum_{k=1}^{m}\delta_{rk}\matrixentry{C}{kj}\\
&=
\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kj}
+
\sum_{k=r^\prime+1}^{m}\delta_{rk}\matrixentry{C}{kj}
\\
&=
\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kj}
+
\sum_{k=r^\prime+1}^{m}\delta_{rk}(0)
\\
&=\sum_{k=1}^{r^\prime}\delta_{rk}\matrixentry{C}{kj}\\
&=\sum_{k=1}^{r^\prime}(0)\matrixentry{C}{kj}\\
&=0
\end{align*}
So row $r$ is a totally zero row, contradicting that this should be the bottommost nonzero row of $B$.  So $r^\prime\geq r$.  By an entirely similar argument, reversing the roles of $B$ and $C$, we would conclude that $r^\prime\leq r$ and therefore $r=r^\prime$.  Thus, combining the first three steps we can say that $D=D^\prime$.  In other words, $B$ and $C$ have the same pivot columns, in the same locations.

\paragraph*{Fourth Step.}  In this final step, we will not argue by contradiction.  Our intent is to determine the values of the $\delta_{ij}$.  Notice that we can use the values of the $d_i$ interchangeably for $B$ and $C$.  Here we go,
\begin{align*}
1
&=\matrixentry{B}{id_i}
\\
&=\sum_{k=1}^{m}\delta_{ik}\matrixentry{C}{kd_i}\\
&=
\delta_{ii}\matrixentry{C}{id_i}
+
\sum_{\substack{k=1\\k\neq i}}^{m}\delta_{ik}\matrixentry{C}{kd_i}
\\
&=
\delta_{ii}(1)
+
\sum_{\substack{k=1\\k\neq i}}^{m}\delta_{ik}(0)
\\
&=\delta_{ii}
\end{align*}
and for $\ell\neq i$
\begin{align*}
0
&=\matrixentry{B}{id_\ell}
\\
&=\sum_{k=1}^{m}\delta_{ik}\matrixentry{C}{kd_\ell}\\
&=
\delta_{i\ell}\matrixentry{C}{\ell d_\ell}
+
\sum_{\substack{k=1\\k\neq\ell}}^{m}\delta_{ik}\matrixentry{C}{kd_\ell}
\\
&=
\delta_{i\ell}(1)
+
\sum_{\substack{k=1\\k\neq\ell}}^{m}\delta_{ik}(0)
\\
&=\delta_{i\ell}
\end{align*}
Finally, having determined the values of the $\delta_{ij}$, we can show that $B=C$.  For $1\leq i\leq m$, $1\leq j\leq n$,
\begin{align*}
\matrixentry{B}{ij}
&=\sum_{k=1}^{m}\delta_{ik}\matrixentry{C}{kj}\\
&=
\delta_{ii}\matrixentry{C}{ij}
+
\sum_{\substack{k=1\\k\neq i}}^{m}\delta_{ik}\matrixentry{C}{kj}
\\
&=
(1)\matrixentry{C}{ij}
+
\sum_{\substack{k=1\\k\neq i}}^{m}(0)\matrixentry{C}{kj}\\
&=\matrixentry{C}{ij}
\end{align*}
So $B$ and $C$ have equal values in every entry, and so are the same matrix.
\end{proof}
\end{theorem}


\end{document}
