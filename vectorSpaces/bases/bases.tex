\documentclass{ximera}

\input{../../preamble.tex}

\title{Bases}

\begin{document}
\begin{abstract}
  A basis is a linearly independent set that spans a vector space.
\end{abstract}
\maketitle

A basis of a vector space is one of the most useful concepts in linear
algebra.  It often provides a concise, finite description of an
infinite vector space.

\begin{definition}[Basis]
  \label{definition:B}
  Suppose $V$ is a vector space.  Then a subset $S\subseteq V$ is a
  \dfn{basis} of $V$ if it is linearly independent and spans $V$.
\end{definition}

So, a basis is a linearly independent spanning set for a vector space.
The requirement that the set spans $V$ insures that $S$ has enough raw
material to build $V$, while the linear independence requirement
insures that we do not have any more raw material than we need.  As we
shall see soon in \ref{section:D}, a basis is a minimal spanning set.

You may have noticed that we used the term basis for some of the
titles of previous theorems (e.g., \ref{theorem:BNS},
\ref{theorem:BCS}, \ref{theorem:BRS}) and if you review each of these
theorems you will see that their conclusions provide linearly
independent spanning sets for sets that we now recognize as subspaces
of $\complex{m}$.  Examples associated with these theorems include
\ref{example:NSLIL}, \ref{example:CSOCD} and \ref{example:IAS}.  As we
will see, these three theorems will continue to be powerful tools,
even in the setting of more general vector spaces.

Furthermore, the archetypes contain an abundance of bases.  For each
coefficient matrix of a system of equations, and for each archetype
defined simply as a matrix, there is a basis for the null space,
\textit{three} bases for the column space, and a basis for the row
space.  For this reason, our subsequent examples will concentrate on
bases for vector spaces other than $\complex{m}$.

Notice that \ref{definition:B} does not preclude a vector space from
having many bases, and this is the case, as hinted above by the
statement that the archetypes contain three bases for the column space
of a matrix.  More generally, we can grab any basis for a vector
space, multiply any one basis vector by a nonzero scalar and create a
slightly different set that is still a basis.  For ``important''
vector spaces, it will be convenient to have a collection of ``nice''
bases.  When a vector space has a single particularly nice basis, it
is sometimes called the \dfn{standard basis} though there is nothing
precise enough about this term to allow us to define it formally---it
is a question of style.  Here are some nice bases for important vector
spaces.

\begin{theorem}[Standard Unit Vectors are a Basis]
  \label{theorem:SUVB}

  The set of standard unit vectors for $\complex{m}$
  (\ref{definition:SUV}), $B=\setparts{\vect{e}_i}{1\leq i\leq m}$ is
  a basis for the vector space $\complex{m}$.

  \begin{proof}
    We must show that the set $B$ is both linearly independent and a
    spanning set for $\complex{m}$.  First, the vectors in $B$ are, by
    \ref{definition:SUV}, the columns of the identity matrix, which we
    know is nonsingular (since it row-reduces to the identity matrix,
    \ref{theorem:NMRRI}).  And the columns of a nonsingular matrix are
    linearly independent by \ref{theorem:NMLIC}.

    Suppose we grab an arbitrary vector from $\complex{m}$, say
    \[
      \vect{v}=\colvector{v_1\\v_2\\v_3\\\vdots\\v_m}.
    \]

    Can we write $\vect{v}$ as a linear combination of the vectors in $B$?  Yes, and quite simply.
    \begin{align*}
      \colvector{v_1\\v_2\\v_3\\\vdots\\v_m}&=
                                              v_1\colvector{1\\0\\0\\\vdots\\0}+
      v_2\colvector{0\\1\\0\\\vdots\\0}+
      v_3\colvector{0\\0\\1\\\vdots\\0}+
      \cdots+
      v_m\colvector{0\\0\\0\\\vdots\\1}\\
      \vect{v}&=v_1\vect{e}_1+v_2\vect{e}_2+v_3\vect{e}_3+\cdots+v_m\vect{e}_m
    \end{align*}
    
    This shows that $\complex{m}\subseteq\spn{B}$, which is sufficient to show that $B$ is a spanning set for $\complex{m}$.
  \end{proof}
\end{theorem}

\begin{example}[Bases for $P_n$]
  The vector space $P_n$ of polynomials with degree at most $n$ has
  the basis
  \[
    B=\set{1,\,x,\,x^2,\,x^3,\,\ldots,\,x^n}.
  \]
  
  Another nice basis for $P_n$ is
  \[
    C=\set{1,\,1+x,\,1+x+x^2,\,1+x+x^2+x^3,\,\ldots,\,1+x+x^2+x^3+\cdots+x^n}.
  \]

  Checking that each of $B$ and $C$ is a linearly independent spanning set are good exercises.
\end{example}

\begin{example}[A basis for the vector space of matrices]
  In the vector space $M_{mn}$ of matrices (\ref{example:VSM})  define the matrices $B_{k\ell}$, $1\leq k\leq m$, $1\leq\ell\leq n$ by
  \[
    \matrixentry{B_{k\ell}}{ij}=\begin{cases}
      1&\text{if }k=i,\,\ell=j\\
      0&\text{otherwise}
    \end{cases}
  \]
  
  So these matrices have entries that are all zeros, with the exception of a single entry that is one.  The set of all $mn$ of them,
  \[
    B=\setparts{B_{k\ell}}{1\leq k\leq m,\ 1\leq\ell\leq n}
  \]
  forms a basis for $M_{mn}$.
\end{example}

The bases described above will often be convenient ones to work with.
However a basis does not have to obviously look like a basis.

\begin{example}[A basis for a subspace of $P_4$]
  We showed that
  \[
    S=\set{x-2,\,x^2-4x+4,\,x^3-6x^2+12x-8,\,x^4-8x^3+24x^2-32x+16}
  \]
  is a spanning set for $W=\setparts{p(x)}{p\in P_4,\ p(2)=0}$.  We
  will now show that $S$ is also linearly independent in $W$.  Begin
  with a relation of linear dependence,
  \begin{align*}
    0+0x&+0x^2+0x^3+0x^4\\
        &=\alpha_1\left(x-2\right)+\alpha_2\left(x^2-4x+4\right)+\alpha_3\left(x^3-6x^2+12x-8\right)\\
        &\quad\quad +\alpha_4\left(x^4-8x^3+24x^2-32x+16\right)\\
        &=\alpha_4x^4+
          \left(\alpha_3-8\alpha_4\right)x^3+
          \left(\alpha_2-6\alpha_3+24\alpha_4\right)x^2\\
        &\quad\quad +
          \left(\alpha_1-4\alpha_2+12\alpha_3-32\alpha_4\right)x+
          \left(-2\alpha_1+4\alpha_2-8\alpha_3+16\alpha_4\right)
  \end{align*}
  
  Equating coefficients (vector equality in $P_4$) gives the homogeneous system of five equations in four variables,
  \begin{align*}
    \alpha_4&=0\\
    \alpha_3-8\alpha_4&=0\\
    \alpha_2-6\alpha_3+24\alpha_4&=0\\
    \alpha_1-4\alpha_2+12\alpha_3-32\alpha_4&=0\\
    -2\alpha_1+4\alpha_2-8\alpha_3+16\alpha_4&=0\\
  \end{align*}

  We form the coefficient matrix, and row-reduce to obtain a matrix in reduced row-echelon form
  \[
    \begin{bmatrix}
      \leading{1}&0&0&0\\
      0&\leading{1}&0&0\\
      0&0&\leading{1}&0\\
      0&0&0&\leading{1}\\
      0&0&0&0
    \end{bmatrix}
  \]

  With \textit{only} the trivial solution to this homogeneous system,
  we conclude that only scalars that will form a relation of linear
  dependence are the trivial ones, and therefore the set $S$ is
  linearly
  \wordChoice{\choice[correct]{independent}\choice{dependent}}.
  Finally, $S$ has earned the right to be called a basis for $W$.
\end{example}

\begin{example}[A basis for a subspace of $M_{22}$]
  We already discovered that
  \[
    Q=\set{
      \begin{bmatrix}-3&1\\0&0\end{bmatrix},\,
      \begin{bmatrix}1&0\\-4&1\end{bmatrix}
    }
  \]
  is a spanning set for the subspace
  \[
    Z=\setparts{\begin{bmatrix}a&b\\c&d\end{bmatrix}}{a+3b-c-5d=0,\ -2a-6b+3c+14d=0}
  \]
  of the vector space of all $2\times 2$ matrices, $M_{22}$.  If we
  can also determine that $Q$ is linearly independent in $Z$ (or in
  $M_{22}$), then it will qualify as a basis for $Z$.

  Let us begin with a relation of linear dependence.
  \begin{align*}
    \begin{bmatrix}0&0\\0&0\end{bmatrix}
                         &=
                           \alpha_1\begin{bmatrix}-3&1\\0&0\end{bmatrix}+
                                                           \alpha_2\begin{bmatrix}1&0\\-4&1\end{bmatrix}\\
                    &=\begin{bmatrix}
                      -3\alpha_1 +\alpha_2  & \alpha_1\\
                      -4\alpha_2 & \alpha_2
                    \end{bmatrix}
  \end{align*}
  
  Using our definition of matrix equality (\ref{definition:ME}) we
  equate entries and get a homogeneous system of four equations in two
  variables,
  \begin{align*}
    -3\alpha_1 +\alpha_2&=0\\
    \alpha_1&=0\\
    -4\alpha_2&=0\\
    \alpha_2&=0
  \end{align*}

  We could row-reduce the coefficient matrix of this homogeneous
  system, but it is not necessary.  The equations tell us that
  $\alpha_1=\answer{0}$, $\alpha_2=\answer{0}$ is the \textit{only}
  solution to this homogeneous system.  This qualifies the set $Q$ as
  being linearly independent, since the only relation of linear
  dependence is trivial (\ref{definition:LI}).  Therefore $Q$ is a
  basis for $Z$ (\ref{definition:B}).
\end{example}

We have seen that several of the sets associated with a matrix are
subspaces of vector spaces of column vectors.  Specifically these are
the null space (\ref{theorem:NSMS}), column space
(\ref{theorem:CSMS}), row space (\ref{theorem:RSMS}) and left null
space (\ref{theorem:LNSMS}).  As subspaces they are vector spaces
(\ref{definition:S}) and it is natural to ask about bases for these
vector spaces.  \ref{theorem:BNS}, \ref{theorem:BCS},
\ref{theorem:BRS} each have conclusions that provide linearly
independent spanning sets for (respectively) the null space, column
space, and row space.  Notice that each of these theorems contains the
word ``basis'' in its title, even though we did not know the precise
meaning of the word at the time.  To find a basis for a left null
space we can use the definition of this subspace as a null space
(\ref{definition:LNS}) and apply \ref{theorem:BNS}.  Or
\ref{theorem:FS} tells us that the left null space can be expressed as
a row space and we can then use \ref{theorem:BRS}.

\ref{theorem:BS} is another early result that provides a linearly
independent spanning set (i.e., a basis) as its conclusion.  If a
vector space of column vectors can be expressed as a span of a set of
column vectors, then \ref{theorem:BS} can be employed in a
straightforward manner to quickly yield a basis.

\end{document}
