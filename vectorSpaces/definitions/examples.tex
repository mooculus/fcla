\documentclass{ximera}

\input{../../preamble.tex}

\title{Examples of vector spaces}

\begin{document}
\begin{abstract}
  A student must consider many examples of vector spaces to become
  comfortable with the ten vector space properties and to be convinced
  that the multitude of examples justifies (at least initially) making
  such a broad definition.
\end{abstract}
\maketitle

Our aim in this subsection is to give you a storehouse of examples to
work with.  Some of our claims will be justified by reference to
previous theorems, we will prove some facts from scratch, and we will
do one nontrivial example completely.  In other places, our usual
thoroughness will be neglected, so grab paper and pencil and play
along.

\begin{example}[The vector space $\complex{m}$]

  \begin{description}
  \item[Set] $\complex{m}$, all column vectors of size $m$, \ref{definition:VSCV}.
  \item[Equality] Entry-wise, \ref{definition:CVE}.
  \item[Vector Addition] The ``usual'' addition, given in \ref{definition:CVA}.
  \item[Scalar Multiplication] The ``usual'' scalar multiplication, given in \ref{definition:CVSM}.
  \end{description}
  
  Does this set with these operations fulfill the ten properties?  
  \begin{multipleChoice}
    \choice[correct]{Yes}
    \choice{No}
  \end{multipleChoice}
  
  \begin{feedback}[correct]
    Yes!  And by design all we need to do is quote \ref{theorem:VSPCV}.
  \end{feedback}
\end{example}

\begin{example}[The vector space of matrices, $M_{mn}$]
  \begin{description}
  \item[Set] $M_{mn}$, the set of all matrices of size $m\times n$ and entries from $\complexes$, \ref{definition:VSM}.
  \item[Equality] Entry-wise, \ref{definition:ME}.
  \item[Vector Addition]  The ``usual'' addition, given in \ref{definition:MA}.
  \item[Scalar Multiplication] The ``usual'' scalar multiplication, given in \ref{definition:MSM}.
  \end{description}

  Does this set with these operations fulfill the ten properties?  
  \begin{multipleChoice}
    \choice[correct]{Yes}
    \choice{No}
  \end{multipleChoice}
  
  \begin{feedback}[correct]
    Yes!  And again by design all we need to do is quote \ref{theorem:VSPCV}.
  \end{feedback}
\end{example}

So, the set of all matrices of a fixed size forms a vector space.
That entitles us to call a matrix a vector, since a matrix is an
element of a vector space.  For example, if $A,\,B\in M_{34}$ then we
call $A$ and $B$ ``vectors,'' and we even use our previous notation
for column vectors to refer to $A$ and $B$.  So we could legitimately
write expressions like
\[
\vect{u}+\vect{v}=A+B=B+A=\vect{v}+\vect{u}
\]
This could lead to some confusion, but it is not too great a danger.  But it is worth comment.

The previous two examples may be less than satisfying.  We made all
the relevant definitions long ago.  And the required verifications
were all handled by quoting old theorems.  However, it is important to
consider these two examples first.  We have been studying vectors and
matrices carefully, and both objects, along with their operations,
have certain properties in common, as you may have noticed in
comparing \ref{theorem:VSPCV} with \ref{theorem:VSPM}.  Indeed, it is
these two theorems that \textit{motivate} us to formulate the abstract
definition of a vector space, \ref{definition:VS}.  Now, if we prove
some general theorems about vector spaces, we can then instantly apply
the conclusions to \textit{both} $\complex{m}$ and $M_{mn}$.  Notice
too, how we have taken six definitions and two theorems and reduced
them down to two \textit{examples}.  With greater generalization and
abstraction our old ideas get downgraded in stature.

Let us look at some more examples, now considering some new vector spaces.

\begin{example}[The vector space of polynomials, $P_n$]

  Set: $P_n$, the set of all polynomials of degree $n$ or less in the variable $x$ with coefficients from $\complexes$.

  Equality:
  \begin{align*}
    a_0+a_1x+a_2x^2+\cdots+a_nx^n&=b_0+b_1x+b_2x^2+\cdots+b_nx^n\\
    \text{ if and only if }a_i&=b_i\text{ for }0\leq i\leq n
  \end{align*}


  Vector Addition:
  \begin{align*}
    (a_0+a_1x+a_2x^2+\cdots+a_nx^n)+(b_0+b_1x+b_2x^2+\cdots+b_nx^n)=\\
    (a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2+\cdots+(a_n+b_n)x^n
  \end{align*}

  Scalar Multiplication:
  \[
    \alpha(a_0+a_1x+a_2x^2+\cdots+a_nx^n)=(\alpha a_0)+(\alpha a_1)x+(\alpha a_2)x^2+\cdots+(\alpha a_n)x^n
  \]

This set, with these operations, 

  Does this set with these operations fulfill the ten properties?  
  \begin{multipleChoice}
    \choice{does not fulfill the ten properties.}
    \choice[correct]{fulfills the ten properties.}
  \end{multipleChoice}
  
  \begin{feedback}[correct]
    We will not work all the details here.  However, we will make a
    few comments and prove one of the properties.  First, the zero
    vector (\ref{property:Z}) is what you might expect, and you can
    check that it has the required property.
    \[
      \zerovector=0+0x+0x^2+\cdots+0x^n
    \]
    
    The additive inverse (\ref{property:AI}) is also no surprise, though consider how we have chosen to write it.
    \[
      -\left(a_0+a_1x+a_2x^2+\cdots+a_nx^n\right)=(-a_0)+(-a_1)x+(-a_2)x^2+\cdots+(-a_n)x^n
    \]

    Now let us prove the associativity of vector addition
    (\ref{property:AA}).  This is a bit tedious, though necessary.
    Throughout, the plus sign (``+'') does triple-duty.  You might ask
    yourself what each plus sign represents as you work through this
    proof.
    \begin{align*}
      \vect{u}+&(\vect{v}+\vect{w})\\
               &=(a_0+a_1x+\cdots+a_nx^n)+\left((b_0+b_1x+\cdots+b_nx^n)+(c_0+c_1x+\cdots+c_nx^n)\right)\\
               &=(a_0+a_1x+\cdots+a_nx^n)+((b_0+c_0)+(b_1+c_1)x+\cdots+(b_n+c_n)x^n)\\
               &=(a_0+(b_0+c_0))+(a_1+(b_1+c_1))x+\cdots+(a_n+(b_n+c_n))x^n\\
               &=((a_0+b_0)+c_0)+((a_1+b_1)+c_1)x+\cdots+((a_n+b_n)+c_n)x^n\\
               &=((a_0+b_0)+(a_1+b_1)x+\cdots+(a_n+b_n)x^n)+(c_0+c_1x+\cdots+c_nx^n)\\
               &=\left((a_0+a_1x+\cdots+a_nx^n)+(b_0+b_1x+\cdots+b_nx^n)\right)+(c_0+c_1x+\cdots+c_nx^n)\\
               &=(\vect{u}+\vect{v})+\vect{w}
    \end{align*}

    Notice how it is the application of the associativity of the (old)
    addition of complex numbers in the middle of this chain of
    equalities that makes the whole proof happen.  The remainder is
    successive applications of our (new) definition of vector
    (polynomial) addition.  Proving the remainder of the ten
    properties is similar in style and tedium.  You might try proving
    the commutativity of vector addition (\ref{property:C}), or one of
    the distributivity properties (\ref{property:DVA},
    \ref{property:DSA}).
  \end{feedback}
\end{example}

\begin{example}[The vector space of infinite sequences]

Set: $\complex{\infty}=\setparts{(c_0,\,c_1,\,c_2,\,c_3,\,\ldots)}{c_i\in\complexes,\ i\in\mathbb{N}}$.

Equality:
\[
  (c_0,\,c_1,\,c_2,\,\ldots)=(d_0,\,d_1,\,d_2,\,\ldots)\text{ if and only if }c_i=d_i\text{ for all }i\geq 0
\]

Vector Addition:
\[
  (c_0,\,c_1,\,c_2,\,\ldots)+(d_0,\,d_1,\,d_2,\,\ldots)=(c_0+d_0,\,c_1+d_1,\,c_2+d_2,\,\ldots)
\]

Scalar Multiplication:
\[
  \alpha (c_0,\,c_1,\,c_2,\,c_3,\,\ldots)=(\alpha c_0,\,\alpha c_1,\,\alpha c_2,\,\alpha c_3,\,\ldots)
\]

This should remind you of the vector space $\complex{m}$, though now
our lists of scalars are written horizontally with commas as
delimiters and they are allowed to be infinite in length.  What does
the zero vector look like (\ref{property:Z})?  Additive inverses
(\ref{property:AI})?  Can you prove the associativity of vector
addition (\ref{property:AA})?
\end{example}

\begin{example}[The vector space of functions]
Let $X$ be any set.

Set: $F=\setparts{f}{f:X\rightarrow\complexes}$.

Equality: $f=g$ if and only if $f(x)=g(x)$ for all $x\in X$.

Vector Addition: $f+g$ is the function with outputs defined by
$(f+g)(x)=f(x)+g(x)$.

Scalar Multiplication: $\alpha f$ is the function with outputs defined
by $(\alpha f)(x)=\alpha f(x)$.

So this is the set of all functions of one variable that take elements
of the set $X$ to a complex number.  You might have studied functions
of one variable that take a real number to a real number, and that
might be a more natural set to use as $X$.  But since we are allowing
our scalars to be complex numbers, we need to specify that the range
of our functions is the complex numbers.  Study carefully how the
definitions of the operation are made, and think about the different
uses of ``+'' and juxtaposition.  As an example of what is required
when verifying that this is a vector space, consider that the zero
vector (\ref{property:Z}) is the function $z$ whose definition is
$z(x)=0$ for every input $x\in X$.

Vector spaces of functions are very important in mathematics and
physics, where the field of scalars may be the real numbers, so the
ranges of the functions can in turn also be the set of real numbers.
\end{example}

Here is a \textit{unique} example.

\begin{example}[The singleton vector space ]

  Set: $Z=\set{\vect{z}}$.

  Equality: Eh, $\vect{z}} = \vect{z}}$ is all to say.

  Vector Addition:  $\vect{z}+\vect{z}=\vect{z}$.

  Scalar Multiplication: $\alpha\vect{z}=\vect{z}$.

  This should look pretty wild.  First, just what is $\vect{z}$?
  Column vector, matrix, polynomial, sequence, function?  Mineral,
  plant, or animal?  We aren't saying!  $\vect{z}$ just \textit{is}.
  And we have definitions of vector addition and scalar multiplication
  that are sufficient for an occurrence of either that may come along.

  Our only concern is if this set, along with the definitions of two
  operations, fulfills the ten properties of \ref{definition:VS}.  Let
  us check associativity of vector addition (\ref{property:AA}).  For
  all $\vect{u},\,\vect{v},\,\vect{w}\in Z$,
  \begin{align*}
    \vect{u}+(\vect{v}+\vect{w})
    &=\vect{z}+(\vect{z}+\vect{z})\\
    &=\vect{z}+\vect{z}\\
    &=(\vect{z}+\vect{z})+\vect{z}\\
    &=(\vect{u}+\vect{v})+\vect{w}
  \end{align*}

  What is the zero vector in this vector space (\ref{property:Z})?
  With only one element in the set, we do not have much choice.  Is
  $\vect{z}=\zerovector$?  It appears that $\vect{z}$ behaves like the
  zero vector should, so it gets the title.  Maybe now the definition
  of this vector space does not seem so bizarre.  It is a set whose
  only element is the element that behaves like the zero vector, so
  that lone element \textit{is} the zero vector.
\end{example}

Perhaps some of the above definitions and verifications seem obvious
or like splitting hairs, but the next example should convince you that
they \textit{are} necessary.  We will study this one carefully.
Ready?  Check your preconceptions at the door.

\begin{example}[The crazy vector space]

Set: $C=\setparts{(x_1,\,x_2)}{x_1,\,x_2\in\complexes}$.

Vector Addition:  $(x_1,\,x_2)+(y_1,\,y_2)=(x_1+y_1+1,\,x_2+y_2+1)$.

Scalar Multiplication: $\alpha(x_1,\,x_2)=(\alpha x_1+\alpha-1,\,\alpha x_2+\alpha-1)$.

Now, the first thing I hear you say is ``You can't do that!''  And my
response is, ``Oh yes, I can!''  I am free to define my set and my
operations any way I please.  They may not look natural, or even
useful, but we will now verify that they provide us with another
example of a vector space.  And that is enough.  If you are
adventurous, you might try first checking some of the properties
yourself.  What is the zero vector?  Additive inverses?  Can you prove
associativity?  Ready, here we go.

\ref{property:AC}, \ref{property:SC}:  The result of each operation is a pair of complex numbers, so these two closure properties are fulfilled.

\ref{property:C}:
\begin{align*}
  \vect{u}+\vect{v}&=(x_1,\,x_2)+(y_1,\,y_2)=(x_1+y_1+1,\,x_2+y_2+1)\\
                   &=(y_1+x_1+1,\,y_2+x_2+1)=(y_1,\,y_2)+(x_1,\,x_2)\\
                   &=\vect{v}+\vect{u}
\end{align*}

\ref{property:AA}:
\begin{align*}
  \vect{u}+(\vect{v}+\vect{w})&=(x_1,\,x_2)+\left((y_1,\,y_2)+(z_1,\,z_2)\right)\\
                              &=(x_1,\,x_2)+(y_1+z_1+1,\,y_2+z_2+1)\\
                              &=(x_1+(y_1+z_1+1)+1,\,x_2+(y_2+z_2+1)+1)\\
                              &=(x_1+y_1+z_1+2,\,x_2+y_2+z_2+2)\\
                              &=((x_1+y_1+1)+z_1+1,\,(x_2+y_2+1)+z_2+1)\\
                              &=(x_1+y_1+1,\,x_2+y_2+1)+(z_1,\,z_2)\\
                              &=\left((x_1,\,x_2)+(y_1,\,y_2)\right)+(z_1,\,z_2)\\
                              &=\left(\vect{u}+\vect{v}\right)+\vect{w}
\end{align*}

\ref{property:Z}:  The zero vector is ldots  $\zerovector=(-1,\,-1)$.  Now I hear you say, ``No, no, that can't be, it must be $(0,\,0)$!''.  Indulge me for a moment and let us check my proposal.
\[
  \vect{u}+\zerovector=(x_1,\,x_2)+(-1,\,-1)=(x_1+(-1)+1,\,x_2+(-1)+1)=(x_1,\,x_2)=\vect{u}
\]
Feeling better?  Or worse?

\ref{property:AI}:  For each vector, $\vect{u}$, we must locate an additive inverse, $\vect{-u}$.  Here it is, $-(x_1,\,x_2)=(-x_1-2,\,-x_2-2)$.  As odd as it may look, I hope you are withholding judgment.  Check:
\begin{align*}
  \vect{u}+ (\vect{-u})&=(x_1,\,x_2)+(-x_1-2,\,-x_2-2)\\
&=(x_1+(-x_1-2)+1,\,-x_2+(x_2-2)+1)=(-1,\,-1)=\zerovector
\end{align*}

\ref{property:SMA}:
\begin{align*}
\alpha(\beta\vect{u})
&=\alpha(\beta(x_1,\,x_2))\\
&=\alpha(\beta x_1+\beta-1,\,\beta x_2+\beta-1)\\
&=(\alpha(\beta x_1+\beta-1)+\alpha-1,\,\alpha(\beta x_2+\beta-1)+\alpha-1)\\
&=((\alpha\beta x_1+\alpha\beta-\alpha)+\alpha-1,\,(\alpha\beta x_2+\alpha\beta-\alpha)+\alpha-1)\\
&=(\alpha\beta x_1+\alpha\beta-1,\,\alpha\beta x_2+\alpha\beta-1)\\
&=(\alpha\beta)(x_1,\,x_2)\\
&=(\alpha\beta)\vect{u}
\end{align*}



\ref{property:DVA}:  If you have hung on so far, here is where it gets even wilder.  In the next two properties we mix and mash the two operations.
\begin{align*}
\alpha(\vect{u}&+\vect{v})\\
&=\alpha\left((x_1,\,x_2)+(y_1,\,y_2)\right)\\
&=\alpha(x_1+y_1+1,\,x_2+y_2+1)\\
&=(\alpha(x_1+y_1+1)+\alpha-1,\,\alpha(x_2+y_2+1)+\alpha-1)\\
&=(\alpha x_1+\alpha y_1+\alpha+\alpha-1,\,\alpha x_2+\alpha y_2+\alpha+\alpha-1)\\
&=(\alpha x_1+\alpha-1+\alpha y_1+\alpha-1+1,\,\alpha x_2+\alpha-1+\alpha y_2+\alpha-1+1)\\
&=((\alpha x_1+\alpha-1)+(\alpha y_1+\alpha-1)+1,\,(\alpha x_2+\alpha-1)+(\alpha y_2+\alpha-1)+1)\\
&=(\alpha x_1+\alpha-1,\,\alpha x_2+\alpha-1)+(\alpha y_1+\alpha-1,\,\alpha y_2+\alpha-1)\\
&=\alpha(x_1,\,x_2)+\alpha(y_1,\,y_2)\\
&=\alpha\vect{u}+\alpha\vect{v}
\end{align*}




\ref{property:DSA}:
\begin{align*}
(\alpha&+\beta)\vect{u}\\
&=(\alpha+\beta)(x_1,\,x_2)\\
&=((\alpha+\beta)x_1+(\alpha+\beta)-1,\,(\alpha+\beta)x_2+(\alpha+\beta)-1)\\
&=(\alpha x_1+\beta x_1+\alpha+\beta-1,\,\alpha x_2+\beta x_2+\alpha+\beta-1)\\
&=(\alpha x_1+\alpha-1+\beta x_1+\beta-1+1,\,\alpha x_2+\alpha-1+\beta x_2+\beta-1+1)\\
&=((\alpha x_1+\alpha-1)+(\beta x_1+\beta-1)+1,\,(\alpha x_2+\alpha-1)+(\beta x_2+\beta-1)+1)\\
&=(\alpha x_1+\alpha-1,\,\alpha x_2+\alpha-1)+(\beta x_1+\beta-1,\,\beta x_2+\beta-1)\\
&=\alpha(x_1,\,x_2)+\beta(x_1,\,x_2)\\
&=\alpha\vect{u}+\beta\vect{u}
\end{align*}




\ref{property:O}:  After all that, this one is easy, but no less pleasing.
\[
1\vect{u}=1(x_1,\,x_2)=(x_1+1-1,\,x_2+1-1)=(x_1,\,x_2)=\vect{u}
\]




That is it, $C$ is a vector space, as crazy as that may seem.



Notice that in the case of the zero vector and additive inverses, we only had to propose possibilities and then verify that they were the correct choices.  You might try to discover how you would arrive at these choices, though you should understand why the process of discovering them is not a necessary component of the proof itself.



\end{example}

\end{document}
