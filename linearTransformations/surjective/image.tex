\documentclass{ximera}

\input{../../preamble.tex}

\title{Image of a Linear Transformation}

\begin{document}
\begin{abstract}
  Informally, the image is the set of all outputs that the transformation creates when fed every possible input from the domain.
\end{abstract}
\maketitle

For a linear transformation $\ltdefn{T}{U}{V}$, the image is a subset of the codomain $V$.  Informally, it is the set of all outputs that the transformation creates when fed every possible input from the domain.  It will have some natural connections with the column space of a matrix, so we will keep the same notation, and if you think about your objects, then there should be little confusion.  Here is the careful definition.

\begin{definition}[Image of a Linear Transformation]

Suppose $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the \dfn{image} of $T$ is the set
\[
\rng{T}=\setparts{\lteval{T}{\vect{u}}}{\vect{u}\in U}
\]
\end{definition}

\begin{example}
Consider
\[
\ltdefn{T}{\complex{3}}{\complex{5}},\quad
\lteval{T}{\colvector{x_1\\x_2\\x_3}}=
\colvector{-x_1 + x_2 - 3 x_3\\
-x_1 + 2 x_2 - 4 x_3\\
x_1 + x_2 + x_3\\
2 x_1 + 3 x_2 + x_3\\
x_1 + 2 x_3
}
\]


To determine the elements of $\complex{5}$ in $\rng{T}$, find those vectors $\vect{v}$ such that $\lteval{T}{\vect{u}}=\vect{v}$ for some $\vect{u}\in\complex{3}$,
\begin{align*}
\vect{v}&=\lteval{T}{\vect{u}}\\
&=\colvector{-u_1 + u_2 - 3 u_3\\
-u_1 + 2 u_2 - 4 u_3\\
u_1 + u_2 + u_3\\
2 u_1 + 3 u_2 + u_3\\
u_1 + 2 u_3
}\\
&=
\colvector{-u_1\\-u_1\\u_1\\2 u_1\\ u_1}
+
\colvector{u_2\\2u_2\\u_2\\3u_2\\ 0}
+
\colvector{-3u_3\\-4u_3\\u_3\\u_3\\ 2 u_3}\\
&=
u_1\colvector{-1\\-1\\1\\2\\1}
+
u_2\colvector{1\\2\\1\\3\\ 0}
+
u_3\colvector{-3\\-4\\1\\1\\2}
\end{align*}




This says that every output of $T$ (in other words, the vector $\vect{v}$) can be written as a linear combination of the three vectors
\begin{align*}
\colvector{-1\\-1\\1\\2\\1}
&&
\colvector{1\\2\\1\\3\\ 0}
&&
\colvector{-3\\-4\\1\\1\\2}
\end{align*}
using the scalars $u_1,\,u_2,\,u_3$.  Furthermore, since $\vect{u}$ can be any element of $\complex{3}$, every such linear combination is an output.  This means that
\[
\rng{T}=\spn{\set{
\colvector{-1\\-1\\1\\2\\1},\,
\colvector{1\\2\\1\\3\\ 0},\,
\colvector{-3\\-4\\1\\1\\2}
}}
\]




The three vectors in this spanning set for $\rng{T}$ form a linearly dependent set (check this!).  So we can find a more economical presentation by any of the various methods from \ref{section:CRS} and \ref{section:FS}.  We will place the vectors into a matrix as rows, row-reduce, toss out zero rows and appeal to \ref{theorem:BRS}, so we can describe the image of $T$ with a basis,
\[
\rng{T}=\spn{\set{
\colvector{1\\0\\-3\\-7\\-2},\,\colvector{0\\1\\2\\5\\1}
}}
\]

\end{example}

We know that the span of a set of vectors is always a subspace (\ref{theorem:SSS}), so the image computed in \ref{example:RAO} is also a subspace.  This is no accident, the image of a linear transformation is \textit{always} a subspace.



\begin{theorem}[Image of a Linear Transformation is a Subspace]
\label{theorem:RLTS}


Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the image of $T$, $\rng{T}$, is a subspace of $V$.





\begin{proof}
We can apply the three-part test of \ref{theorem:TSS}.  First, $\zerovector_U\in U$ and $\lteval{T}{\zerovector_U}=\zerovector_V$ by \ref{theorem:LTTZZ}, so $\zerovector_V\in\rng{T}$ and we know that the image is nonempty.



Suppose we assume that $\vect{x},\,\vect{y}\in\rng{T}$.  Is $\vect{x}+\vect{y}\in\rng{T}$?  If $\vect{x},\,\vect{y}\in\rng{T}$ then we know there are vectors $\vect{w},\,\vect{z}\in U$ such that $\lteval{T}{\vect{w}}=\vect{x}$ and $\lteval{T}{\vect{z}}=\vect{y}$.  Because $U$ is a vector space, additive closure (\ref{property:AC}) implies that $\vect{w}+\vect{z}\in U$.



Then
\begin{align*}
\lteval{T}{\vect{w}+\vect{z}}&=\lteval{T}{\vect{w}}+\lteval{T}{\vect{z}}&&\ref{definition:LT}\\
&=\vect{x}+\vect{y}&&\text{Definition of $\vect{w}$ and $\vect{z}$}
\end{align*}




So we have found an input, $\vect{w}+\vect{z}$, which when fed into $T$ creates $\vect{x}+\vect{y}$ as an output.  This qualifies $\vect{x}+\vect{y}$ for membership in $\rng{T}$.  So we have additive closure.



Suppose we assume that $\alpha\in\complexes$ and $\vect{x}\in\rng{T}$.  Is $\alpha\vect{x}\in\rng{T}$?  If $\vect{x}\in\rng{T}$, then there is a vector $\vect{w}\in U$ such that $\lteval{T}{\vect{w}}=\vect{x}$.  Because $U$ is a vector space, scalar closure implies that $\alpha\vect{w}\in U$.  Then
\begin{align*}
\lteval{T}{\alpha\vect{w}}&=\alpha\lteval{T}{\vect{w}}&&\ref{definition:LT}\\
&=\alpha\vect{x}&&\text{Definition of $\vect{w}$}
\end{align*}




So we have found an input ($\alpha\vect{w}$) which when fed into $T$ creates $\alpha\vect{x}$ as an output.  This qualifies $\alpha\vect{x}$ for membership in $\rng{T}$.  So we have scalar closure and \ref{theorem:TSS} tells us that $\rng{T}$ is a subspace of $V$.



\end{proof}
\end{theorem}

Let us compute another image, now that we know in advance that it will be a subspace.



\begin{example}
Consider
\[
\ltdefn{T}{\complex{5}}{\complex{3}},\quad
\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4\\x_5}}=
\colvector{2 x_1 + x_2 + 3 x_3 - 4 x_4 + 5 x_5\\
x_1 - 2 x_2 + 3 x_3 - 9 x_4 + 3 x_5\\
3 x_1 + 4 x_3 - 6 x_4 + 5 x_5}
\]

To determine the elements of $\complex{3}$ in $\rng{T}$, find those vectors $\vect{v}$ such that $\lteval{T}{\vect{u}}=\vect{v}$ for some $\vect{u}\in\complex{5}$,
\begin{align*}
\vect{v}&=\lteval{T}{\vect{u}}\\
&=
\colvector{
2 u_1 + u_2 + 3 u_3 - 4 u_4 + 5 u_5\\
u_1 - 2 u_2 + 3 u_3 - 9 u_4 + 3 u_5\\
3 u_1 + 4 u_3 - 6 u_4 + 5 u_5}\\
&=
\colvector{2u_1\\u_1\\3u_1}+
\colvector{u_2\\-2u_2\\0}+
\colvector{3u_3\\3u_3\\4u_3}+
\colvector{-4u_4\\-9u_4\\-6u_4}+
\colvector{5u_5\\3u_5\\5u_5}\\
&=
u_1\colvector{2\\1\\3}+
u_2\colvector{1\\-2\\0}+
u_3\colvector{3\\3\\4}+
u_4\colvector{-4\\-9\\-6}+
u_5\colvector{5\\3\\5}\\
\end{align*}




This says that every output of $T$ (in other words, the vector $\vect{v}$) can be written as a linear combination of the five vectors
\begin{align*}
\colvector{2\\1\\3}&&
\colvector{1\\-2\\0}&&
\colvector{3\\3\\4}&&
\colvector{-4\\-9\\-6}&&
\colvector{5\\3\\5}
\end{align*}
using the scalars $u_1,\,u_2,\,u_3,\,u_4,\,u_5$.  Furthermore, since $\vect{u}$ can be any element of $\complex{5}$, every such linear combination is an output.  This means that
\[
\rng{T}=\spn{\set{
\colvector{2\\1\\3},\,
\colvector{1\\-2\\0},\,
\colvector{3\\3\\4},\,
\colvector{-4\\-9\\-6},\,
\colvector{5\\3\\5}
}}
\]


The five vectors in this spanning set for $\rng{T}$ form a linearly dependent set (\ref{theorem:MVSLD}).  So we can find a more economical presentation by any of the various methods from \ref{section:CRS} and \ref{section:FS}.  We will place the vectors into a matrix as rows, row-reduce, toss out zero rows and appeal to \ref{theorem:BRS}, so we can describe the image of $T$ with a (nice) basis,
\[
\rng{T}=\spn{\set{
\colvector{1\\0\\0},\,\colvector{0\\1\\0},\,\colvector{0\\0\\1}
}} = \complex{3}\]

\end{example}

In contrast to injective linear transformations having small (trivial) kernels (\ref{theorem:KILT}), surjective linear transformations have large images, as indicated in the next theorem.



\begin{theorem}[Image of a Surjective Linear Transformation]
\label{theorem:RSLT}


Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then $T$ is surjective if and only if the image of $T$ equals the codomain, $\rng{T}=V$.


\begin{proof}
($\Rightarrow$) By \ref{definition:RLT}, we know that $\rng{T}\subseteq V$.  To establish the reverse inclusion, assume $\vect{v}\in V$.  Then since $T$ is surjective (\ref{definition:SLT}), there exists a vector $\vect{u}\in U$ so that $\lteval{T}{\vect{u}}=\vect{v}$.  However, the existence of $\vect{u}$ gains $\vect{v}$ membership in $\rng{T}$, so $V\subseteq\rng{T}$.  Thus, $\rng{T}=V$.



($\Leftarrow$)  To establish that $T$ is surjective, choose $\vect{v}\in V$.  Since we are assuming that $\rng{T}=V$, $\vect{v}\in\rng{T}$.  This says there is a vector $\vect{u}\in U$ so that $\lteval{T}{\vect{u}}=\vect{v}$, i.e.,  $T$ is surjective.



\end{proof}
\end{theorem}

\begin{example}

Recall that
b\[\ltdefn{T}{\complex{5}}{\complex{5}},\quad
\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4\\x_5}}=
\colvector{-2 x_1 + 3 x_2 + 3 x_3 - 6 x_4 + 3 x_5\\
-16 x_1 + 9 x_2 + 12 x_3 - 28 x_4 + 28 x_5\\
-19 x_1 + 7 x_2 + 14 x_3 - 32 x_4 + 37 x_5\\
-21 x_1 + 9 x_2 + 15 x_3 - 35 x_4 + 39 x_5\\
-9 x_1 + 5 x_2 + 7 x_3 - 16 x_4 + 16 x_5}
\]
was shown to be not surjective by constructing a vector in the codomain where no element of the domain could be used to evaluate the linear transformation to create the output, thus violating \ref{definition:SLT}.  Just where did this vector come from?

The short answer is that the vector
\[
\vect{v}=\colvector{-1\\2\\3\\-1\\4}
\]
was constructed to lie outside of the image of $T$.  How was this accomplished?  First, the image of $T$ is given by
\[
\rng{T}=\spn{\set{
\colvector{1\\0\\0\\0\\1},\,\colvector{0\\1\\0\\0\\-1},\,
\colvector{0\\0\\1\\0\\-1},\,\colvector{0\\0\\0\\1\\2}
}}
\]

Suppose an element of the image $\vect{v^*}$ has its first 4 components equal to $-1$, $2$, $3$, $-1$, in that order.  Then to be an element of $\rng{T}$, we would have
\[
\vect{v^*}=(-1)\colvector{1\\0\\0\\0\\1}+(2)\colvector{0\\1\\0\\0\\-1}+(3)
\colvector{0\\0\\1\\0\\-1}+(-1)\colvector{0\\0\\0\\1\\2}
=\colvector{-1\\2\\3\\-1\\-8}
\]


So the only vector in the image with these first four components specified, must have $-8$ in the fifth component.  To set the fifth component to any other value (say, 4) will result in a vector ($\vect{v}$ in \ref{example:NSAQ})  outside of the image.  Any attempt to find an input for $T$ that will produce $\vect{v}$ as an output will be doomed to failure.



Whenever the image of a linear transformation is not the whole codomain, we can employ this device and conclude that the linear transformation is not surjective.
This is another way of viewing \ref{theorem:RSLT}.  For a surjective linear transformation, the image is all of the codomain and there is no choice for a vector $\vect{v}$ that lies in $V$, yet not in the image.  For every one of the archetypes that is not surjective, there is an example presented of exactly this form.



\end{example}

\begin{example}

The image of
\[
\ltdefn{T}{\complex{3}}{\complex{5}},\quad
\lteval{T}{\colvector{x_1\\x_2\\x_3}}=
\colvector{-x_1 + x_2 - 3 x_3\\
-x_1 + 2 x_2 - 4 x_3\\
x_1 + x_2 + x_3\\
2 x_1 + 3 x_2 + x_3\\
x_1 + 2 x_3
}
\]
 was determined to be
\[
\rng{T}=\spn{\colvector{1\\0\\-3\\-7\\-2},\,\colvector{0\\1\\2\\5\\1}}
\]
a subspace of dimension 2 in $\complex{5}$.  Since $\rng{T}\neq\complex{5}$, \ref{theorem:RSLT} says 
\begin{multipleChoice}
\choice{$T$ is surjective.}
\choice[correct]{$T$ is not surjective.}
\end{multipleChoice}
\end{example}

\begin{example}

The image of 
\[
\ltdefn{T}{\complex{5}}{\complex{3}},\quad
\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4\\x_5}}=
\colvector{2 x_1 + x_2 + 3 x_3 - 4 x_4 + 5 x_5\\
x_1 - 2 x_2 + 3 x_3 - 9 x_4 + 3 x_5\\
3 x_1 + 4 x_3 - 6 x_4 + 5 x_5}
\]
 was computed to be
\[
\rng{T}=\spn{\set{
\colvector{1\\0\\0},\,\colvector{0\\1\\0},\,\colvector{0\\0\\1}
}}\]

Since the basis for this subspace is the set of standard unit vectors for $\complex{3}$ (\ref{theorem:SUVB}), we have $\rng{T}=\complex{3}$ and by \ref{theorem:RSLT}, we may conclude
\begin{multipleChoice}
\choice[correct]{$T$ is surjective.}
\choice{$T$ is not surjective.}
\end{multipleChoice}

\end{example}

\end{document}
