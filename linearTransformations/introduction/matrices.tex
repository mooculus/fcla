\documentclass{ximera}

\input{../../preamble.tex}

\title{Matrices and Linear Transformations}

\begin{document}
\begin{abstract}
Matrices are deeply related to linear transformations.
\end{abstract}
\maketitle

If you give me a matrix, then I can quickly build you a linear transformation.  Always.  First a motivating example and then the theorem.

\begin{example}[Linear transformation from a matrix]

Let
\[
A=
\begin{bmatrix}
3&-1&8&1\\
2&0&5&-2\\
1&1&3&-7
\end{bmatrix}
\]
and define a function $\ltdefn{P}{\complex{4}}{\complex{3}}$ by
\[
\lteval{P}{\vect{x}}=A\vect{x}
\]


So we are using an old friend, the matrix-vector product (\ref{definition:MVP}) as a way to convert a vector with 4 components into a vector with 3 components.  Applying \ref{definition:MVP} allows us to write the defining formula for $P$ in a slightly different form,
\[
\lteval{P}{\vect{x}}=A\vect{x}=
\begin{bmatrix}
3&-1&8&1\\
2&0&5&-2\\
1&1&3&-7
\end{bmatrix}
\colvector{x_1\\x_2\\x_3\\x_4}
=
x_1\colvector{3\\2\\1}+
x_2\colvector{-1\\0\\1}+
x_3\colvector{8\\5\\3}+
x_4\colvector{1\\-2\\-7}
\]




So we recognize the action of the function $P$ as using the components of the vector ($x_1,\,x_2,\,x_3,\,x_4$) as scalars to form the output of $P$ as a linear combination of the four columns of the matrix $A$, which are all members of $\complex{3}$, so the result is a vector in $\complex{3}$.  We can rearrange this expression further, using our definitions of operations in $\complex{3}$ (\ref{section:VO}).
\begin{align*}
\lteval{P}{\vect{x}}
&=A\vect{x}&&\text{Definition of $P$}\\
&=
x_1\colvector{3\\2\\1}+
x_2\colvector{-1\\0\\1}+
x_3\colvector{8\\5\\3}+
x_4\colvector{1\\-2\\-7}&&\ref{definition:MVP}\\
&=
\colvector{3x_1\\2x_1\\x_1}+
\colvector{-x_2\\0\\x_2}+
\colvector{8x_3\\5x_3\\3x_3}+
\colvector{x_4\\-2x_4\\-7x_4}&&\ref{definition:CVSM}\\
&=\colvector{3x_1-x_2+8x_3+x_4\\2x_1+5x_3-2x_4\\x_1+x_2+3x_3-7x_4}&&\ref{definition:CVA}
\end{align*}




You might recognize this final expression as being similar in style to some previous examples (\ref{example:ALT}) and some linear transformations defined in the archetypes (\ref{archetype:M} through \ref{archetype:R}).  But the expression that says the output of this linear transformation is a linear combination of the columns of $A$ is probably the most powerful way of thinking about examples of this type.



Almost forgot --- we should verify that $P$ is indeed a linear transformation.  This is easy with two matrix properties from \ref{section:MM}.
\begin{align*}
\lteval{P}{\vect{x}+\vect{y}}
&=A\left(\vect{x}+\vect{y}\right)&&\text{Definition of $P$}\\
&=A\vect{x}+A\vect{y}&&\ref{theorem:MMDAA}\\
&=\lteval{P}{\vect{x}}+\lteval{P}{\vect{y}}&&\text{Definition of $P$}
<intertext>and</intertext>
\lteval{P}{\alpha\vect{x}}
&=A\left(\alpha\vect{x}\right)&&\text{Definition of $P$}\\
&=\alpha\left(A\vect{x}\right)&&\ref{theorem:MMSMM}\\
&=\alpha\lteval{P}{\vect{x}}&&\text{Definition of $P$}
\end{align*}




So by \ref{definition:LT}, $P$ is a linear transformation.



\end{example}

So the multiplication of a vector by a matrix ``transforms'' the input vector into an output vector, possibly of a different size, by performing a linear combination.  And this transformation happens in a ``linear'' fashion.  This ``functional'' view of the matrix-vector product is the most important shift you can make right now in how you think about linear algebra.  Here is the theorem, whose proof is very nearly an exact copy of the verification in the last example.



\begin{theorem}
\label{theorem:MBLT}
[Matrices Build Linear Transformations]

Suppose that $A$ is an $m\times n$ matrix.  Define a function $\ltdefn{T}{\complex{n}}{\complex{m}}$ by $\lteval{T}{\vect{x}}=A\vect{x}$.  Then $T$ is a linear transformation.





\begin{proof}

\begin{align*}
\lteval{T}{\vect{x}+\vect{y}}
&=A\left(\vect{x}+\vect{y}\right)&&\text{Definition of $T$}\\
&=A\vect{x}+A\vect{y}&&\ref{theorem:MMDAA}\\
&=\lteval{T}{\vect{x}}+\lteval{T}{\vect{y}}&&\text{Definition of $T$}
<intertext>and</intertext>
\lteval{T}{\alpha\vect{x}}
&=A\left(\alpha\vect{x}\right)&&\text{Definition of $T$}\\
&=\alpha\left(A\vect{x}\right)&&\ref{theorem:MMSMM}\\
&=\alpha\lteval{T}{\vect{x}}&&\text{Definition of $T$}
\end{align*}




So by \ref{definition:LT}, $T$ is a linear transformation.



\end{proof}
\end{theorem}

So \ref{theorem:MBLT} gives us a rapid way to construct linear transformations.  Grab an $m\times n$ matrix $A$, define $\lteval{T}{\vect{x}}=A\vect{x}$ and \ref{theorem:MBLT} tells us that $T$ is a linear transformation from $\complex{n}$ to $\complex{m}$, without any further checking.



We can turn \ref{theorem:MBLT} around.  You give me a linear transformation and I will give you a matrix.



\begin{example}
[Matrix from a linear transformation]

Define the function $\ltdefn{R}{\complex{3}}{\complex{4}}$ by
\[
\lteval{R}{\colvector{x_1\\x_2\\x_3}}=
\colvector{2x_1-3x_2+4x_3\\x_1+x_2+x_3\\-x_1+5x_2-3x_3\\x_2-4x_3}
\]



You could verify that $R$ is a linear transformation by applying the definition, but we will instead massage the expression defining a typical output until we recognize the form of a known class of linear transformations.
\begin{align*}
\lteval{R}{\colvector{x_1\\x_2\\x_3}}&=
\colvector{2x_1-3x_2+4x_3\\x_1+x_2+x_3\\-x_1+5x_2-3x_3\\x_2-4x_3}\\
&=
\colvector{2x_1\\x_1\\-x_1\\0}+
\colvector{-3x_2\\x_2\\5x_2\\x_2}+
\colvector{4x_3\\x_3\\-3x_3\\-4x_3}&&\ref{definition:CVA}\\
&=
x_1\colvector{2\\1\\-1\\0}+
x_2\colvector{-3\\1\\5\\1}+\
x_3\colvector{4\\1\\-3\\-4}&&\ref{definition:CVSM}\\
&=
\begin{bmatrix}
2&-3&4\\
1&1&1\\
-1&5&-3\\
0&1&-4
\end{bmatrix}
\colvector{x_1\\x_2\\x_3}&&\ref{definition:MVP}
\end{align*}




So if we define the matrix
\[
B=
\begin{bmatrix}
2&-3&4\\
1&1&1\\
-1&5&-3\\
0&1&-4
\end{bmatrix}
\]
then $\lteval{R}{\vect{x}}=B\vect{x}$.  By \ref{theorem:MBLT}, we can easily recognize $R$ as a linear transformation since it has the form described in the hypothesis of the theorem.



\end{example}

\ref{example:MFLT} was not an accident.  Consider any one of the archetypes where both the domain and codomain are sets of column vectors (\ref{archetype:M} through \ref{archetype:R}) and you should be able to mimic the previous example.  Here is the theorem, which is notable since it is our first occasion to use the full power of the defining properties of a linear transformation when our hypothesis includes a linear transformation.



\begin{theorem}
\label{theorem:MLTCV}
[Matrix of a Linear Transformation, Column Vectors]

<indexlocation index="linear transformation!matrix of" />
Suppose that $\ltdefn{T}{\complex{n}}{\complex{m}}$ is a linear transformation.  Then there is an $m\times n$ matrix $A$ such that $\lteval{T}{\vect{x}}=A\vect{x}$.





\begin{proof}
The conclusion says a certain matrix exists.  What better way to prove something exists than to actually build it?  So our proof will be constructive (\ref{technique:C}), and the procedure that we will use abstractly in the proof can be used concretely in specific examples.



Let $\vectorlist{e}{n}$ be the columns of the identity matrix of size $n$, $I_n$ (\ref{definition:SUV}).  Evaluate the linear transformation $T$ with each of these standard unit vectors as an input, and record the result.  In other words, define $n$ vectors in $\complex{m}$, $\vect{A}_i$, $1\leq i\leq n$ by
\[
\vect{A}_i=\lteval{T}{\vect{e}_i}
\]




Then package up these vectors as the columns of a matrix
\[
A=\matrixcolumns{A}{n}
\]




Does $A$ have the desired properties?  First, $A$ is clearly an $m\times n$ matrix.  Then
\begin{align*}
\lteval{T}{\vect{x}}
&=\lteval{T}{I_n\vect{x}}
&&\ref{theorem:MMIM}\\
&=\lteval{T}{\matrixcolumns{e}{n}\vect{x}}
&&\ref{definition:SUV}\\
&=\lteval{T}{
\vectorentry{\vect{x}}{1}\vect{e}_1+
\vectorentry{\vect{x}}{2}\vect{e}_2+
\vectorentry{\vect{x}}{3}\vect{e}_3+
\cdots+
\vectorentry{\vect{x}}{n}\vect{e}_n
}
&&\ref{definition:MVP}\\
&=
\lteval{T}{\vectorentry{\vect{x}}{1}\vect{e}_1}+
\lteval{T}{\vectorentry{\vect{x}}{2}\vect{e}_2}+
\lteval{T}{\vectorentry{\vect{x}}{3}\vect{e}_3}+
\cdots+
\lteval{T}{\vectorentry{\vect{x}}{n}\vect{e}_n}
&&\ref{definition:LT}\\
&=
\vectorentry{\vect{x}}{1}\lteval{T}{\vect{e}_1}+
\vectorentry{\vect{x}}{2}\lteval{T}{\vect{e}_2}+
\vectorentry{\vect{x}}{3}\lteval{T}{\vect{e}_3}+
\cdots+
\vectorentry{\vect{x}}{n}\lteval{T}{\vect{e}_n}
&&\ref{definition:LT}\\
&=
\vectorentry{\vect{x}}{1}{\vect{A}_1}+
\vectorentry{\vect{x}}{2}{\vect{A}_2}+
\vectorentry{\vect{x}}{3}{\vect{A}_3}+
\cdots+
\vectorentry{\vect{x}}{n}{\vect{A}_n}
&&\text{Definition of $\vect{A}_i$}\\
&=A\vect{x}
&&\ref{definition:MVP}
\end{align*}
as desired.



\end{proof}
\end{theorem}

So if we were to restrict our study of linear transformations to those where the domain and codomain are both vector spaces of column vectors (\ref{definition:VSCV}), every matrix leads to a linear transformation of this type (\ref{theorem:MBLT}), while every such linear transformation leads to a matrix (\ref{theorem:MLTCV}).  So matrices and linear transformations are fundamentally the same.  We call the matrix $A$ of \ref{theorem:MLTCV} the \dfn{matrix representation} of $T$.



We have defined linear transformations for more general vector spaces than just $\complex{m}$. Can we extend this correspondence between linear transformations and matrices to more general linear transformations (more general domains and codomains)?  Yes, and this is the main theme of \ref{chapter:R}.  Stay tuned.  For now, let us illustrate \ref{theorem:MLTCV} with an example.



\begin{example}
[Matrix of a linear transformation]

Suppose $\ltdefn{S}{\complex{3}}{\complex{4}}$ is defined by
\[
\lteval{S}{\colvector{x_1\\x_2\\x_3}}=\colvector{3x_1-2x_2+5x_3\\x_1+x_2+x_3\\9x_1-2x_2+5x_3\\4x_2}
\]




Then
\begin{align*}
\vect{C}_1&=\lteval{S}{\vect{e_1}}=\lteval{S}{\colvector{1\\0\\0}}=\colvector{3\\1\\9\\0}\\
\vect{C}_2&=\lteval{S}{\vect{e_2}}=\lteval{S}{\colvector{0\\1\\0}}=\colvector{-2\\1\\-2\\4}\\
\vect{C}_3&=\lteval{S}{\vect{e_3}}=\lteval{S}{\colvector{0\\0\\1}}=\colvector{5\\1\\5\\0}
\end{align*}
so define
\[
C=\left[C_1|C_2|C_3\right]=
\begin{bmatrix}
3&-2&5\\
1&1&1\\
9&-2&5\\
0&4&0
\end{bmatrix}
\]
and \ref{theorem:MLTCV} guarantees that $\lteval{S}{\vect{x}}=C\vect{x}$.

As an illuminating exercise, let $\vect{z}=\colvector{2\\-3\\3}$ and compute $\lteval{S}{\vect{z}}$ two different ways.  First, return to the definition of $S$ and evaluate $\lteval{S}{\vect{z}}$ directly.  Then do the matrix-vector product $C\vect{z}$.  In both cases you should obtain the vector $\lteval{S}{\vect{z}}=\colvector{27\\2\\39\\-12}$.

\end{example}

\begin{question}
Define two linear transformations, $\ltdefn{T}{\complex{4}}{\complex{3}}$ and $\ltdefn{S}{\complex{3}}{\complex{2}}$ by
\begin{align*}
\lteval{S}{\colvector{x_1\\x_2\\x_3}}
&=
\colvector{
x_1-2x_2+3x_3\\
5x_1+4x_2+2x_3
}
&
\lteval{T}{\colvector{x_1\\x_2\\x_3\\x_4}}
&=
\colvector{
-x_1+3x_2+x_3+9x_4\\
2x_1+x_3+7x_4\\
4x_1+2x_2+x_3+2x_4
}
\end{align*}
Using the proof of \ref{theorem:MLTCV}, the matrix representations of the three linear transformations $T$ is
\[
\begin{bmatrix}
\answer{1} & -2 & 3\\
\answer{5} & 4 & 2
\end{bmatrix}
\]
and the matrix representation of $S$ is
\[
\begin{bmatrix}
-1 & \answer{3} & 1 & 9 \\
 2 & \answer{0} & 1 & 7 \\
 4 & \answer{2} & 1 & 2
\end{bmatrix}
\]
and the matrix representation of $\compose{S}{T}$ is
\[
\begin{bmatrix}
\answer{7} & 9 & 2 & 1 \\
\answer{11} & 19 & 11 & 77
\end{bmatrix}.
\]
\end{question}


\end{document}
