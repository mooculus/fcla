\documentclass{ximera}

\input{../../preamble.tex}

\title{Invertible Linear Transformations}

\begin{document}
\begin{abstract}
  We bring together the twin properties of injectivity and
  surjectivity and consider linear transformations with both of these
  properties.
\end{abstract}
\maketitle

In this activity w

\begin{definition}[Identity Linear Transformation]

The \dfn{identity linear transformation} on the vector space $W$ is defined as
\[
\ltdefn{I_W}{W}{W},\quad\quad \lteval{I_W}{\vect{w}}=\vect{w}
\]

\end{definition}

Informally, $I_W$ is the ``do-nothing'' function.  You should check that $I_W$ is really a linear transformation, as claimed, and then compute its kernel and range to see that it is both injective and surjective.  All of these facts should be straightforward to verify.  With this in hand we can make our main definition.


\begin{definition}
[Invertible Linear Transformations]
Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  If there is a function $\ltdefn{S}{V}{U}$ such that
\begin{align*}
\compose{S}{T}&=I_U & \compose{T}{S}&=I_V
\end{align*}
then $T$ is \dfn{invertible}.  In this case, we call $S$ the \dfn{inverse} of $T$ and write $S=\ltinverse{T}$.

\end{definition}

Informally, a linear transformation $T$ is invertible if there is a companion linear transformation, $S$, which ``undoes'' the action of $T$.  When the two linear transformations are applied consecutively (composition), in either order, the result is to have no real effect.  It is entirely analogous to squaring a positive number and then taking its (positive) square root.



Here is an example of a linear transformation that is invertible.  As usual at the beginning of a section, do not be concerned with where $S$ came from, just understand how it illustrates \ref{definition:IVLT}.



\begin{example}
Consider
\[
\ltdefn{T}{P_3}{M_{22}},\quad\lteval{T}{a+bx+cx^2+dx^3}=
\begin{bmatrix}
a+b & a-2c\\
d & b-d
\end{bmatrix}
\]

Define the function $\ltdefn{S}{M_{22}}{P_3}$ by
\[
\lteval{S}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}=(a - c - d)+ (c + d)x +\frac{1}{2}(a - b - c - d)x^2+cx^3
\]

Then
\begin{align*}
&\lteval{\left(\compose{T}{S}\right)}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}\\
&\quad\quad=\lteval{T}{\lteval{S}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}}\\
&\quad\quad=\lteval{T}{(a - c - d)+ (c + d)x +\frac{1}{2}(a - b - c - d)x^2+cx^3}\\
&\quad\quad=\begin{bmatrix}
(a - c - d)+ (c + d)&(a - c - d)-2(\frac{1}{2}(a - b - c - d))\\c&(c + d)-c
\end{bmatrix}\\
&\quad\quad=\begin{bmatrix}a&b\\c&d\end{bmatrix}\\
&\quad\quad=\lteval{I_{M_{22}}}{\begin{bmatrix}a&b\\c&d\end{bmatrix}}
<intertext>and</intertext>
&\lteval{\left(\compose{S}{T}\right)}{a+bx+cx^2+dx^3}\\
&\quad\quad=\lteval{S}{\lteval{T}{a+bx+cx^2+dx^3}}\\
&\quad\quad=\lteval{S}{\begin{bmatrix}
a+b&a-2c\\d&b-d
\end{bmatrix}}\\
&\quad\quad=((a+b)-d-(b-d))+
(d+(b-d))x\\
&\quad\quad+\left(\frac{1}{2}((a+b)-(a-2c)-d-(b-d))\right)x^2+
(d)x^3\\
&\quad\quad=a+bx+cx^2+dx^3\\
&\quad\quad=\lteval{I_{P_3}}{a+bx+cx^2+dx^3}
\end{align*}

For now, understand why these computations show
\begin{multipleChoice}
\choice[correct]{that $T$ is invertible.}
\choice{that $T$ is not invertible.}
\end{multipleChoice}

\begin{feedback}[correct]
Moreover, $S=T^{-1}$.  Maybe even be amazed by how $S$ works so perfectly in concert with $T$!  We will see later just how to arrive at the correct form of $S$ (when it is possible).
\end{feedback}

\end{example}

It can be as instructive to study a linear transformation that is not invertible.

\begin{example}

Consider the linear transformation $\ltdefn{T}{\complex{3}}{M_{22}}$ defined by
\[
\lteval{T}{\colvector{a\\b\\c}}=\begin{bmatrix}
a-b&2a+2b+c\\3a+b+c&-2a-6b-2c
\end{bmatrix}
\]


Suppose we were to search for an inverse function $\ltdefn{S}{M_{22}}{\complex{3}}$.



First verify that the $2\times 2$ matrix
$A=\begin{bmatrix}
5&3\\8&2
\end{bmatrix}
$
is not in the range of $T$.  This will amount to finding an input to $T$, $\colvector{a\\b\\c}$, such that
\begin{align*}
a-b&=5\\
2a+2b+c&=3\\
3a+b+c&=8\\
-2a-6b-2c&=2
\end{align*}




As this system of equations is inconsistent, there is no input column vector, and $A\not\in\rng{T}$.  How should we define $\lteval{S}{A}$?  Note that
\[
\lteval{T}{\lteval{S}{A}}=\lteval{\left(\compose{T}{S}\right)}{A}=
\lteval{I_{M_{22}}}{A}=A
\]




So any definition we would provide for $\lteval{S}{A}$ must then be a column vector that $T$ sends to $A$ and we would have $A\in\rng{T}$, contrary to the definition of $T$.  This is enough to see that there is no function $S$ that will allow us to conclude that $T$ is invertible, since we cannot provide a consistent definition for $\lteval{S}{A}$ if we assume $T$ is invertible.



Even though we now know that $T$ is not invertible, let us not leave this example just yet.  Check that
\begin{align*}
\lteval{T}{\colvector{1\\-2\\4}}&=\begin{bmatrix}3&2\\5&2\end{bmatrix}=B&
\lteval{T}{\colvector{0\\-3\\8}}&=\begin{bmatrix}3&2\\5&2\end{bmatrix}=B
\end{align*}




How would we define $\lteval{S}{B}$?
\begin{align*}
\lteval{S}{B}&=\lteval{S}{\lteval{T}{\colvector{1\\-2\\4}}}
=\lteval{\left(\compose{S}{T}\right)}{\colvector{1\\-2\\4}}
=\lteval{I_{\complex{3}}}{\colvector{1\\-2\\4}}=\colvector{1\\-2\\4}
<intertext>or</intertext>
\lteval{S}{B}&=\lteval{S}{\lteval{T}{\colvector{0\\-3\\8}}}
=\lteval{\left(\compose{S}{T}\right)}{\colvector{0\\-3\\8}}
=\lteval{I_{\complex{3}}}{\colvector{0\\-3\\8}}=\colvector{0\\-3\\8}
\end{align*}




Which definition should we provide for $\lteval{S}{B}$?  Both are necessary.  But then $S$ is not a function.  So we have a second reason to know that there is no function $S$ that will allow us to conclude that $T$ is invertible.  It happens that there are infinitely many column vectors that $S$ would have to take to $B$.  Construct the kernel of $T$,
\[
\krn{T}=\spn{\set{\colvector{-1\\-1\\4}}}
\]




Now choose either of the two inputs used above for $T$ and add to it a scalar multiple of the basis vector for the kernel of $T$.  For example,
\[
\vect{x}=\colvector{1\\-2\\4}+(-2)\colvector{-1\\-1\\4}=\colvector{3\\0\\-4}
\]
then verify that $\lteval{T}{\vect{x}}=B$.  Practice creating a few more inputs for $T$ that would be sent to $B$, and see why it is hopeless to think that we could ever provide a reasonable definition for $\lteval{S}{B}$!  There is a ``whole subspace's worth'' of values that $\lteval{S}{B}$ would have to take on.



\end{example}

In \ref{example:ANILT} you may have noticed that $T$ is not surjective, since the matrix $A$ was not in the range of $T$.  And $T$ is not injective since there are two different input column vectors that $T$ sends to the matrix $B$.  Linear transformations $T$ that are not surjective lead to putative inverse functions $S$ that are undefined on inputs outside of the range of $T$.  Linear transformations $T$ that are not injective lead to putative inverse functions $S$ that are multiply-defined on each of their inputs.  We will formalize these ideas in \ref{theorem:ILTIS}.



But first notice in \ref{definition:IVLT} that we only require the inverse (when it exists) to be a function.  When it does exist, it too is a linear transformation.



\begin{theorem}[Inverse of a Linear Transformation is a Linear Transformation]
\label{theorem:ILTLT}


Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then the function $\ltdefn{\ltinverse{T}}{V}{U}$ is a linear transformation.




\begin{proof}
We work through verifying \ref{definition:LT} for $\ltinverse{T}$, using the fact that $T$ is a linear transformation to obtain the second equality in each half of the proof.  To this end, suppose $\vect{x},\,\vect{y}\in V$ and $\alpha\in\complexes$.
\begin{align*}
\lteval{\ltinverse{T}}{\vect{x}+\vect{y}}&=
\lteval{\ltinverse{T}}{\lteval{T}{\lteval{\ltinverse{T}}{\vect{x}}}+\lteval{T}{\lteval{\ltinverse{T}}{\vect{y}}}}
&&\ref{definition:IVLT}\\
&=\lteval{\ltinverse{T}}{\lteval{T}{\lteval{\ltinverse{T}}{\vect{x}}+\lteval{\ltinverse{T}}{\vect{y}}}}&&\ref{definition:LT}\\
&=\lteval{\ltinverse{T}}{\vect{x}}+\lteval{\ltinverse{T}}{\vect{y}}
&&\ref{definition:IVLT}
\end{align*}




Now check the second defining property of a linear transformation for $\ltinverse{T}$,
\begin{align*}
\lteval{\ltinverse{T}}{\alpha\vect{x}}&=
\lteval{\ltinverse{T}}{\alpha\lteval{T}{\lteval{\ltinverse{T}}{\vect{x}}}}
&&\ref{definition:IVLT}\\
&=\lteval{\ltinverse{T}}{\lteval{T}{\alpha\lteval{\ltinverse{T}}{\vect{x}}}}
&&\ref{definition:LT}\\
&=\alpha\lteval{\ltinverse{T}}{\vect{x}}
&&\ref{definition:IVLT}
\end{align*}




So $\ltinverse{T}$ fulfills the requirements of \ref{definition:LT} and is therefore a linear transformation.


\end{proof}
\end{theorem}

So when $T$ has an inverse, $\ltinverse{T}$ is also a linear transformation.  Furthermore, $\ltinverse{T}$ is an invertible linear transformation and \textit{its} inverse is what you might expect.



\begin{theorem}[Inverse of an Invertible Linear Transformation]
\label{theorem:IILT}


Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then $\ltinverse{T}$ is an invertible linear transformation and $\ltinverse{\left(\ltinverse{T}\right)}=T$.





\begin{proof}
Because $T$ is invertible, \ref{definition:IVLT} tells us there is a function $\ltdefn{\ltinverse{T}}{V}{U}$ such that
\begin{align*}
\compose{\ltinverse{T}}{T}&=I_U & \compose{T}{\ltinverse{T}}&=I_V
\end{align*}




Additionally, \ref{theorem:ILTLT} tells us that $\ltinverse{T}$ is more than just a function, it is a linear transformation.  Now view these two statements as properties of the linear transformation $\ltinverse{T}$.  In light of \ref{definition:IVLT}, they together say that $\ltinverse{T}$ is invertible (let $T$ play the role of $S$ in the statement of the definition).  Furthermore, the inverse of $\ltinverse{T}$ is then $T$, i.e.,  $\ltinverse{\left(\ltinverse{T}\right)}=T$.



\end{proof}
\end{theorem}

\end{document}
