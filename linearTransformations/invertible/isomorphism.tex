\documentclass{ximera}

\input{../../preamble.tex}

\title{Structure and Isomorphism}

\begin{document}
\begin{abstract}
  When two different vector spaces have an invertible linear
  transformation defined between them, then we can translate questions
  about linear combinations (spans, linear independence, bases,
  dimension) from the first vector space to the second.
\end{abstract}
\maketitle

A vector space is defined (\ref{definition:VS}) as a set of objects (``vectors'') endowed with a definition of vector addition ($+$) and a definition of scalar multiplication (written with juxtaposition).  Many of our definitions about vector spaces involve linear combinations (\ref{definition:LC}), such as the span of a set (\ref{definition:SS}) and linear independence (\ref{definition:LI}).  Other definitions are built up from these ideas, such as bases (\ref{definition:B}) and dimension (\ref{definition:D}).  The defining properties of a linear transformation require that a function ``respect'' the operations of the two vector spaces that are the domain and the codomain (\ref{definition:LT}).  Finally, an invertible linear transformation is one that can be ``undone'' --- it has a companion that reverses its effect.  In this subsection we are going to begin to roll all these ideas into one.



A vector space has ``structure'' derived from definitions of the two operations and the requirement that these operations interact in ways that satisfy the ten properties of \ref{definition:VS}.  When two different vector spaces have an invertible linear transformation defined between them, then we can translate questions about linear combinations (spans, linear independence, bases, dimension) from the first vector space to the second.  The answers obtained in the second vector space can then be translated back, via the inverse linear transformation, and interpreted in the setting of the first vector space.  We say that these invertible linear transformations ``preserve structure.''  And we say that the two vector spaces are ``structurally the same.''  The precise term is ``isomorphic,'' from Greek meaning ``of the same form.''  Let us begin to try to understand this important concept.



\begin{definition}[Isomorphic Vector Spaces]

Two vector spaces $U$ and $V$ are \dfn{isomorphic} if there exists an invertible linear transformation $T$ with domain $U$ and codomain $V$, $\ltdefn{T}{U}{V}$.  In this case, we write $U\isomorphic V$, and the linear transformation $T$ is known as an \dfn{isomorphism} between $U$ and $V$.


\end{definition}

A few comments on this definition.  First, be careful with your language (\ref{technique:L}).  Two vector spaces are isomorphic, or not.  It is a yes/no situation and the term only applies to a pair of vector spaces.  Any invertible linear transformation can be called an isomorphism, it is a term that applies to functions.  Second, given a pair of vector spaces there might be several different isomorphisms between the two vector spaces.  But it only takes the existence of one to call the pair isomorphic.  Third,  $U$ isomorphic to $V$, or $V$ isomorphic to $U$?  It does not matter, since the inverse linear transformation will provide the needed isomorphism in the ``opposite'' direction.  Being ``isomorphic to'' is an equivalence relation on the set of all vector spaces (see \ref{theorem:SER} for a reminder about equivalence relations).



\begin{example}
Consider a certain linear transformation from $P_3$ to $M_{22}$, namely
\[\ltdefn{T}{P_3}{M_{22}},\quad\lteval{T}{a+bx+cx^2+dx^3}=
\begin{bmatrix}
a+b & a-2c\\
d & b-d
\end{bmatrix}
\]

Since it is injective and surjective, \ref{theorem:ILTIS} tells us that it is 
\begin{multipleChoice}
  \choice[correct]{an invertible linear transformation.}
  \choice{an non-invertible linear transformation.}
\end{multipleChoice}

\begin{feedback}[correct]
By \ref{definition:IVS} we say $P_3$ and $M_{22}$ are isomorphic.
\end{feedback}


At a basic level, the term ``isomorphic'' is nothing more than a codeword for the presence of an invertible linear transformation.  However, it is also a description of a powerful idea, and this power only becomes apparent in the course of studying examples and related theorems.  In this example, we are led to believe that there is nothing ``structurally'' different about $P_3$ and $M_{22}$.  In a certain sense they are the same.  Not equal, but the same.  One is as good as the other.  One is just as interesting as the other.



Here is an extremely basic application of this idea.  Suppose we want to compute the following linear combination of polynomials in $P_3$,
\[
5(2+3x-4x^2+5x^3)+(-3)(3-5x+3x^2+x^3)
\]




Rather than doing it straight-away (which is very easy), we will apply the transformation $T$ to convert into a linear combination of matrices, and then compute in $M_{22}$ according to the definitions of the vector space operations there (\ref{example:VSM}),
\begin{align*}
&\lteval{T}{5(2+3x-4x^2+5x^3)+(-3)(3-5x+3x^2+x^3)}\\
&=5\lteval{T}{2+3x-4x^2+5x^3}+(-3)\lteval{T}{3-5x+3x^2+x^3}&&\ref{theorem:LTLC}\\
&=5
\begin{bmatrix}
5 & 10\\5 & -2
\end{bmatrix}
+(-3)
\begin{bmatrix}
-2 & -3\\1 & -6
\end{bmatrix}&&\text{Definition of $T$}\\
&=
\begin{bmatrix}
31 & 59\\22 & 8
\end{bmatrix}&&\text{Operations in $M_{22}$}
\end{align*}




Now we will translate our answer back to $P_3$ by applying $\ltinverse{T}$, which we demonstrated in \ref{example:AIVLT},
\[
<archetypepart acro="V" part="ltinverse" />\]




We compute,
\[
\lteval{\ltinverse{T}}{
\begin{bmatrix}
31 & 59\\22 & 8
\end{bmatrix}
}
=1+30x-29x^2+22x^3
\]
which is, as expected, exactly what we would have computed for the original linear combination had we just used the definitions of the operations in $P_3$ (\ref{example:VSP}).  Notice this is meant only as an \textit{illustration} and not a suggested route for doing this particular computation.



\end{example}

In \ref{example:IVSAV} we avoided a computation in $P_3$ by a conversion of the computation to a new vector space, $M_{22}$, via an invertible linear transformation (also known as an isomorphism).  Here is a diagram meant to illustrate the more general situation of two vector spaces, $U$ and $V$, and an invertible linear transformation, $T$.  The diagram is simply about a sum of two vectors from $U$, rather than a more involved linear combination.
%\begin{image}
%\begin{tikzpicture}[ampersand replacement=\&]
%\matrix (m) [matrix of math nodes, row sep=5em, column sep=10em, text height=1.5ex, text depth=0.25ex]
%{ \vect{u}_1,\,\vect{u}_2 \& T(\vect{u}_1),\,T(\vect{u}_2) \\
%\vect{u}_1+\vect{u}_2 \& T(\vect{u}_1+\vect{u}_2)=T(\vect{u}_1)+T(\vect{u}_2)\\};
%\path[->]
%(m-1-1) edge[thick] node[auto] {$T$}             (m-1-2)
%(m-1-2) edge[thick] node[auto] {$+$}             (m-2-2)
%(m-1-1) edge[thick] node[auto] {$+$}             (m-2-1)
%(m-2-2) edge[thick] node[auto] {$\ltinverse{T}$} (m-2-1);
%\end{tikzpicture}
%\end{image}
To understand this diagram, begin in the upper-left corner, and by going straight down we can compute the sum of the two vectors using the addition for the vector space $U$.  The more circuitous alternative, in the spirit of \ref{example:IVSAV}, is to begin in the upper-left corner and then proceed clockwise around the other three sides of the rectangle.  Notice that the vector addition is accomplished using the addition in the vector space $V$.  Then, because $T$ is a linear transformation, we can say that the result of $\lteval{T}{\vect{u}_1}+\lteval{T}{\vect{u}_2}$ is  equal to $\lteval{T}{\vect{u}_1+\vect{u}_2}$.  Then the key feature is to recognize that applying $\ltinverse{T}$ obviously converts the second version of this result into the sum in the lower-left corner.  So there are two routes to the sum $\vect{u}_1+\vect{u}_2$, each employing an addition from a different vector space, but one is ``direct'' and the other is ``roundabout''.  You might try designing a similar diagram for the case of scalar multiplication (see \ref{diagram:DLTM}) or for a full linear combination.

Checking the dimensions of two vector spaces can be a quick way to establish that they are not isomorphic.  Here is the theorem.

\begin{theorem}[Isomorphic Vector Spaces have Equal Dimension]
\label{theorem:IVSED}

Suppose $U$ and $V$ are isomorphic vector spaces.  Then $\dimension{U}=\dimension{V}$.

\begin{proof}
If $U$ and $V$ are isomorphic, there is an invertible linear transformation $\ltdefn{T}{U}{V}$ (\ref{definition:IVS}).  $T$ is injective by \ref{theorem:ILTIS} and so by \ref{theorem:ILTD}, $\dimension{U}\leq\dimension{V}$.  Similarly, $T$ is surjective by \ref{theorem:ILTIS} and so by \ref{theorem:SLTD}, $\dimension{U}\geq\dimension{V}$.  The net effect of these two inequalities is that $\dimension{U}=\dimension{V}$.



\end{proof}
\end{theorem}

The contrapositive of \ref{theorem:IVSED} says that if $U$ and $V$ have different dimensions, then they are not isomorphic.  Dimension is the simplest ``structural'' characteristic that will allow you to distinguish non-isomorphic vector spaces.  For example $P_6$ is not isomorphic to $M_{34}$ since their dimensions (7 and 12, respectively) are not equal.  With tools developed in \ref{section:VR} we will be able to establish that the converse of \ref{theorem:IVSED} is true.  Think about that one for a moment.



\end{document}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
