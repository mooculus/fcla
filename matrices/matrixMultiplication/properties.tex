\documentclass{ximera}

\input{../../preamble.tex}

\title{Properties of Matrix Multiplication}

\begin{document}
\begin{abstract}
  We collect properties of matrix multiplication and its interaction
  with the zero matrix, the identity matrix, matrix addition, scalar
  matrix multiplication, the inner product, conjugation, and the
  transpose.
\end{abstract}
\maketitle

These are great proofs to practice with, so try to concoct the proofs
before reading them, they will get progressively more complicated as
we go.

\begin{theorem}[Matrix Multiplication and the Zero Matrix]
\label{theorem:MMZM}

Suppose $A$ is an $m\times n$ matrix.  Then
\begin{enumerate}
\item $A\zeromatrix_{n\times p}=\zeromatrix_{m\times p}$
\item $\zeromatrix_{p\times m}A=\zeromatrix_{p\times n}$
\end{enumerate}

\begin{proof}
  We will prove (1) and leave (2) to you.  Entry-by-entry, for $1\leq i\leq m$, $1\leq j\leq p$,
  \begin{align*}
    \matrixentry{A\zeromatrix_{n\times p}}{ij}
    &=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{\zeromatrix_{n\times p}}{kj}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{n}\matrixentry{A}{ik}0
    &&\ref{definition:ZM}\\
    &=\sum_{k=1}^{n}0\\
    &=0
    &&\ref{property:ZCN}\\
    &=\matrixentry{\zeromatrix_{m\times p}}{ij}
    &&\ref{definition:ZM}
  \end{align*}

  So by the definition of matrix equality (\ref{definition:ME}), the matrices $A\zeromatrix_{n\times p}$ and $\zeromatrix_{m\times p}$ are equal.
\end{proof}
\end{theorem}

\begin{theorem}
  \label{theorem:MMIM}
  [Matrix Multiplication and Identity Matrix]
  
  Suppose $A$ is an $m\times n$ matrix.  Then
  \begin{enumerate}
  \item $AI_n=A$
  \item $I_mA=A$
  \end{enumerate}

  \begin{proof}
    Again, we will prove (1) and leave (2) to you.  Entry-by-entry,    For $1\leq i\leq m$, $1\leq j\leq n$,
    \begin{align*}
      \matrixentry{AI_n}{ij}=&
                               \sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{I_n}{kj}
      &&\ref{theorem:EMP}\\
                             &=\matrixentry{A}{ij}\matrixentry{I_n}{jj}+\sum_{\substack{k=1\\k\neq j}}^{n}\matrixentry{A}{ik}\matrixentry{I_n}{kj}
                             &&\ref{property:CACN}\\
                             &=\matrixentry{A}{ij}(1)+\sum_{k=1, k\neq j}^{n}\matrixentry{A}{ik}(0)
      &&\ref{definition:IM}\\
                             &=\matrixentry{A}{ij}+\sum_{k=1, k\neq j}^{n}0\\
                             &=\matrixentry{A}{ij}
    \end{align*}
    
    So the matrices $A$ and $AI_n$ are equal, entry-by-entry, and by the definition of matrix equality (\ref{definition:ME}) we can say they are equal matrices.

\end{proof}
\end{theorem}

It is this theorem that gives the identity matrix its name.  It is a
matrix that behaves with matrix multiplication like the scalar 1 does
with scalar multiplication.  To multiply by the identity matrix is to
have no effect on the other matrix.

\begin{theorem}[Matrix Multiplication Distributes Across Addition]
  \label{theorem:MMDAA}

  Suppose $A$ is an $m\times n$ matrix and $B$ and $C$ are $n\times p$ matrices and $D$ is a $p\times s$ matrix.    Then
\begin{enumerate}
\item $A(B+C)=AB+AC$
\item $(B+C)D=BD+CD$
\end{enumerate}

\begin{proof}
  We will do (1), you do (2).  Entry-by-entry, for $1\leq i\leq m$, $1\leq j\leq p$,
  \begin{align*}
    \matrixentry{A(B+C)}{ij}
    &=
      \sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B+C}{kj}&&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{n}\matrixentry{A}{ik}(\matrixentry{B}{kj}+\matrixentry{C}{kj})&&\ref{definition:MA}\\
    &=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}+\matrixentry{A}{ik}\matrixentry{C}{kj}
                                                            &&\ref{property:DCN}\\
    &=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}+\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{C}{kj}&&\ref{property:CACN}\\
    &=\matrixentry{AB}{ij}+\matrixentry{AC}{ij}&&\ref{theorem:EMP}\\
    &=\matrixentry{AB+AC}{ij}&&\ref{definition:MA}
  \end{align*}

  So the matrices $A(B+C)$ and $AB+AC$ are equal, entry-by-entry, and by the definition of matrix equality (\ref{definition:ME}) we can say they are equal matrices.

\end{proof}
\end{theorem}

\begin{theorem}[Matrix Multiplication and Scalar Matrix Multiplication]
  \label{theorem:MMSMM}

  Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Let $\alpha$ be a scalar.  Then $\alpha(AB)=(\alpha A)B=A(\alpha B)$.

  \begin{proof}
    These are equalities of matrices.  We will do the first one, the second is similar and will be good practice for you.    For $1\leq i\leq m$, $1\leq j\leq p$,
    \begin{align*}
      \matrixentry{\alpha(AB)}{ij}
      &=\alpha\matrixentry{AB}{ij}&&\ref{definition:MSM}\\
      &=\alpha\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}&&\ref{theorem:EMP}\\
      &=\sum_{k=1}^{n}\alpha\matrixentry{A}{ik}\matrixentry{B}{kj}&&\ref{property:DCN}\\
      &=\sum_{k=1}^{n}\matrixentry{\alpha A}{ik}\matrixentry{B}{kj}&&\ref{definition:MSM}\\
      &=\matrixentry{(\alpha A)B}{ij}&&\ref{theorem:EMP}
    \end{align*}
    
    So the matrices $\alpha(AB)$ and $(\alpha A)B$ are equal, entry-by-entry, and by the definition of matrix equality (\ref{definition:ME}) we can say they are equal matrices.
    
  \end{proof}
\end{theorem}

\begin{theorem}[Matrix Multiplication is Associative]
  \label{theorem:MMA}

  Suppose $A$ is an $m\times n$ matrix, $B$ is an $n\times p$ matrix and $D$ is a $p\times s$ matrix.  Then  $A(BD)=(AB)D$.

  \begin{proof}
    A matrix equality, so we will go entry-by-entry, no surprise there.    For $1\leq i\leq m$, $1\leq j\leq s$,
    \begin{align*}
      \matrixentry{A(BD)}{ij}
      &=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{BD}{kj}
      &&\ref{theorem:EMP}\\
      &=\sum_{k=1}^{n}\matrixentry{A}{ik}\left(\sum_{\ell=1}^{p}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}\right)
      &&\ref{theorem:EMP}\\
      &=\sum_{k=1}^{n}\sum_{\ell=1}^{p}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}
      &&\ref{property:DCN}\\
    \end{align*}
    We can switch the order of the summation since these are finite sums,
    \begin{align*}
      &=\sum_{\ell=1}^{p}\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\matrixentry{D}{\ell j}
      &&\ref{property:CACN}
    \end{align*}
    As $\matrixentry{D}{\ell j}$ does not depend on the index $k$, we can use distributivity to move it outside of the inner sum,
    \begin{align*}
      &=\sum_{\ell=1}^{p}\matrixentry{D}{\ell j}\left(\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{k\ell}\right)
      &&\ref{property:DCN}\\
      &=\sum_{\ell=1}^{p}\matrixentry{D}{\ell j}\matrixentry{AB}{i\ell}
      &&\ref{theorem:EMP}\\
      &=\sum_{\ell=1}^{p}\matrixentry{AB}{i\ell}\matrixentry{D}{\ell j}
      &&\ref{property:CMCN}\\
      &=\matrixentry{(AB)D}{ij}&&\ref{theorem:EMP}
    \end{align*}
    
    So the matrices $(AB)D$ and $A(BD)$ are equal, entry-by-entry, and by the definition of matrix equality (\ref{definition:ME}) we can say they are equal matrices.
    
  \end{proof}
\end{theorem}

Since \ref{theorem:MMA} says matrix multipication is associative, it
means we do not have to be careful about the order in which we perform
matrix multiplication, nor how we parenthesize an expression with just
several matrices multiplied togther.  So this is where we draw the
line on explaining every last detail in a proof.  We will frequently
add, remove, or rearrange parentheses with no comment.  Indeed, I only
see about a dozen places where \ref{theorem:MMA} is cited in a proof.
You could try to count how many times we \textit{avoid} making a
reference to this theorem.

The statement of our next theorem is technically inaccurate.  If we
upgrade the vectors $\vect{u},\,\vect{v}$ to matrices with a single
column, then the expression $\transpose{\conjugate{\vect{u}}}\vect{v}$
is a $1\times 1$ matrix, though we will treat this small matrix as if
it was simply the scalar quantity in its lone entry.  When we apply
\ref{theorem:MMIP} there should not be any confusion.  Notice that if
we treat a column vector as a matrix with a single column, then we can
also construct the adjoint of a vector, though we will not make this a
common practice.

\begin{theorem}[Matrix Multiplication and Inner Products]
\label{theorem:MMIP}

If we consider the vectors $\vect{u},\,\vect{v}\in\complex{m}$ as $m\times 1$ matrices then
\begin{align*}
  \innerproduct{\vect{u}}{\vect{v}}
  &=\transpose{\conjugate{\vect{u}}}\vect{v}
    =\adjoint{\vect{u}}\vect{v}
\end{align*}

\begin{proof}
  
  \begin{align*}
    \innerproduct{\vect{u}}{\vect{v}}
    &=\sum_{k=1}^{m}\conjugate{\vectorentry{\vect{u}}{k}}\vectorentry{\vect{v}}{k}
    &&\ref{definition:IP}\\
    &=\sum_{k=1}^{m}\conjugate{\matrixentry{\vect{u}}{k1}}\matrixentry{\vect{v}}{k1}
    &&\text{Column vectors as matrices}\\
    &=\sum_{k=1}^{m}\matrixentry{\conjugate{\vect{u}}}{k1}\matrixentry{\vect{v}}{k1}
    &&\ref{definition:CCM}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{\conjugate{\vect{u}}}}{1k}\matrixentry{\vect{v}}{k1}
    &&\ref{definition:TM}\\
    &=\matrixentry{\transpose{\conjugate{\vect{u}}}\vect{v}}{11}
    &&\ref{theorem:EMP}\\
  \end{align*}

  To finish we just blur the distinction between a $1\times 1$ matrix ($\transpose{\conjugate{\vect{u}}}\vect{v}$) and its lone entry.
  
\end{proof}
\end{theorem}

\begin{theorem}[Matrix Multiplication and Complex Conjugation]
\label{theorem:MMCC}

Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\conjugate{AB}=\conjugate{A}\,\conjugate{B}$.

\begin{proof}
  To obtain this matrix equality, we will work entry-by-entry.  For $1\leq i\leq m$, $1\leq j\leq p$,
  \begin{align*}
    \matrixentry{\conjugate{AB}}{ij}
    &=\conjugate{\matrixentry{AB}{ij}}
    &&\ref{definition:CCM}\\
    &=\conjugate{\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{n}\conjugate{\matrixentry{A}{ik}\matrixentry{B}{kj}}
    &&\ref{theorem:CCRA}\\
    &=\sum_{k=1}^{n}\conjugate{\matrixentry{A}{ik}}\,\,\conjugate{\matrixentry{B}{kj}}
    &&\ref{theorem:CCRM}\\
    &=\sum_{k=1}^{n}\matrixentry{\conjugate{A}}{ik}\matrixentry{\conjugate{B}}{kj}
    &&\ref{definition:CCM}\\
    &=\matrixentry{\conjugate{A}\,\conjugate{B}}{ij}
    &&\ref{theorem:EMP}
  \end{align*}

  So the matrices $\conjugate{AB}$ and $\conjugate{A}\,\conjugate{B}$
  are equal, entry-by-entry, and by the definition of matrix equality
  (\ref{definition:ME}) we can say they are equal matrices.
  
\end{proof}
\end{theorem}

Another theorem in this style, and it is a good one.  If you have been practicing with the previous proofs you should be able to do this one yourself.

\begin{theorem}[Matrix Multiplication and Transposes]
\label{theorem:MMT}

Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\transpose{(AB)}=\transpose{B}\transpose{A}$.

\begin{proof}
  This theorem may be surprising but if we check the sizes of the
  matrices involved, then maybe it will not seem so far-fetched.
  First, $AB$ has size $m\times p$, so its transpose has size
  $p\times m$.  The product of $\transpose{B}$ with $\transpose{A}$ is
  a $p\times n$ matrix times an $n\times m$ matrix, also resulting in
  a $p\times m$ matrix.  So at least our objects are compatible for
  equality (and would not be, in general, if we did not reverse the
  order of the matrix multiplication).

  Here we go again, entry-by-entry.  For $1\leq i\leq m$, $1\leq j\leq p$,
  \begin{align*}
    \matrixentry{\transpose{(AB)}}{ji}=&\matrixentry{AB}{ij}
    &&\ref{definition:TM}\\
                                       &=\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}
    &&\ref{theorem:EMP}\\
                                       &=\sum_{k=1}^{n}\matrixentry{B}{kj}\matrixentry{A}{ik}
    &&\ref{property:CMCN}\\
                                       &=\sum_{k=1}^{n}\matrixentry{\transpose{B}}{jk}\matrixentry{\transpose{A}}{ki}
    &&\ref{definition:TM}\\
                                       &=\matrixentry{\transpose{B}\transpose{A}}{ji}
    &&\ref{theorem:EMP}
  \end{align*}
  
  So the matrices $\transpose{(AB)}$ and $\transpose{B}\transpose{A}$
  are equal, entry-by-entry, and by the definition of matrix equality
  (\ref{definition:ME}) we can say they are equal matrices.

\end{proof}
\end{theorem}

This theorem seems odd at first glance, since we have to switch the order of $A$ and $B$.  But if we simply consider the sizes of the matrices involved, we can see that the switch is necessary for this reason alone.  That the individual entries of the products then come along to be equal is a bonus.

As the adjoint of a matrix is a composition of a conjugate and a transpose, its interaction with matrix multiplication is similar to that of a transpose.  Here is the last of our long list of basic properties of matrix multiplication.

\begin{theorem}[Matrix Multiplication and Adjoints]
\label{theorem:MMAD}

Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\adjoint{(AB)}=\adjoint{B}\adjoint{A}$.

\begin{proof}

\begin{align*}
\adjoint{(AB)}
&=\transpose{\left(\conjugate{AB}\right)}&&\ref{definition:A}\\
&=\transpose{\left(\conjugate{A}\,\conjugate{B}\right)}&&\ref{theorem:MMCC}\\
&=\transpose{\left(\conjugate{B}\right)}\transpose{\left(\conjugate{A}\right)}&&\ref{theorem:MMT}\\
&=\adjoint{B}\adjoint{A}&&\ref{definition:A}
\end{align*}

\end{proof}
\end{theorem}

Notice how none of these proofs above relied on writing out huge
general matrices with lots of ellipses (``ldots '') and trying to
formulate the equalities a whole matrix at a time.  This messy
business is a ``proof technique'' to be avoided at all costs.  Notice
too how the proof of \ref{theorem:MMAD} does not use an entry-by-entry
approach, but simply builds on previous results about matrix
multiplication's interaction with conjugation and transposes.

These theorems, along with \ref{theorem:VSPM} and the other results in
\ref{section:MO}, give you the ``rules'' for how matrices interact
with the various operations we have defined on matrices (addition,
scalar multiplication, matrix multiplication, conjugation, transposes
and adjoints).  Use them and use them often.  But do not try to do
anything with a matrix that you do not have a rule for.  Together, we
would informally call all these operations, and the attendant
theorems, ``the algebra of matrices.''  Notice, too, that every column
vector is just a $n\times 1$ matrix, so these theorems apply to column
vectors also.  Finally, these results, taken as a whole, may make us
feel that the definition of matrix multiplication is not so unnatural.

\end{document}
