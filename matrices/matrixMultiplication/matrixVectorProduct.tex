\documentclass{ximera}

\input{../../preamble.tex}

\title{Matrix-Vector Product}

\begin{document}
\begin{abstract}
  In this section we mix linear combinations of vectors with our
  understanding of matrices to produce an operation known as ``matrix
  multiplication.''
\end{abstract}
\maketitle

We know how to add vectors and how to multiply them by scalars.
Together, these operations give us the possibility of making linear
combinations.  Similarly, we know how to add matrices and how to
multiply matrices by scalars.  In this section we mix all these ideas
together and produce an operation known as ``matrix multiplication.''
This will lead to some results that are both surprising and central.
We begin with a definition of how to multiply a vector by a matrix.

We have repeatedly seen the importance of forming linear combinations
of the columns of a matrix.  As one example of this, the oft-used
\ref{theorem:SLSLC}, said that every solution to a system of linear
equations gives rise to a linear combination of the column vectors of
the coefficient matrix that equals the vector of constants.  This
theorem, and others, motivate the following central definition.


\begin{definition}[Matrix-Vector Product]

  Suppose $A$ is an $m\times n$ matrix with columns
  $\vectorlist{A}{n}$ and $\vect{u}$ is a vector of size $n$.  Then
  the \dfn{matrix-vector product} of $A$ with $\vect{u}$ is
  the linear combination
  \[
    A\vect{u}=
    \vectorentry{\vect{u}}{1}\vect{A}_1+
    \vectorentry{\vect{u}}{2}\vect{A}_2+
    \vectorentry{\vect{u}}{3}\vect{A}_3+
    \cdots+
    \vectorentry{\vect{u}}{n}\vect{A}_n
  \]
\end{definition}

So, the matrix-vector product is yet another version of
``multiplication,'' at least in the sense that we have yet again
overloaded juxtaposition of two symbols as our notation.  Remember
your objects, an $m\times n$ matrix times a vector of size $n$ will
create a vector of size $m$.  So if $A$ is rectangular, then the size
of the vector changes.  With all the linear combinations we have
performed so far, this computation should now seem second nature.

\begin{example}[A matrix times a vector]

  Consider
  \begin{align*}
    A=
    \begin{bmatrix}
      1 & 4 & 2 & 3 & 4\\
      -3 & 2 & 0 & 1 & -2\\
      1 & 6 & -3 & -1 & 5
    \end{bmatrix}
         &&
            \vect{u}=\colvector{2\\1\\-2\\3\\-1}
  \end{align*}
  
  Then
  \[
    A\vect{u}=
    \colvector{\answer{7}\\\answer{1}\\\answer{6}}.
  \]
  \begin{hint}
    \[
      A\vect{u}=
      2\colvector{1\\-3\\1}+
      1\colvector{4\\2\\6}+
      (-2)\colvector{2\\0\\-3}+
      3\colvector{3\\1\\-1}+
      (-1)\colvector{4\\-2\\5}
      =
      \colvector{\answer{7}\\\answer{1}\\\answer{6}}.
    \]
  \end{hint}
\end{example}

We can now represent systems of linear equations compactly with a
matrix-vector product (\ref{definition:MVP}) and column vector
equality (\ref{definition:CVE}).  This finally yields a very popular
alternative to our unconventional $\linearsystem{A}{\vect{b}}$
notation.

\begin{theorem}[Systems of Linear Equations as Matrix Multiplication]
\label{theorem:SLEMM}

The set of solutions to the linear system $\linearsystem{A}{\vect{b}}$
equals the set of solutions for $\vect{x}$ in the vector equation
$A\vect{x}=\vect{b}$.

\begin{proof}
  This theorem says that two sets (of solutions) are equal.  So we
  need to show that one set of solutions is a subset of the other, and
  vice versa (\ref{definition:SE}).  Let $\vectorlist{A}{n}$ be the
  columns of $A$.  Both of these set inclusions then follow from the
  following chain of equivalences (\ref{technique:E}),
  \begin{align*}
    &\vect{x}\text{ is a solution to }\linearsystem{A}{\vect{b}}\\
    &\iff
      \vectorentry{\vect{x}}{1}\vect{A}_1+
      \vectorentry{\vect{x}}{2}\vect{A}_2+
      \vectorentry{\vect{x}}{3}\vect{A}_3+
      \cdots+
      \vectorentry{\vect{x}}{n}\vect{A}_n
      =\vect{b}&&\ref{theorem:SLSLC}\\
    &\iff
      \vect{x}\text{ is a solution to }A\vect{x}=\vect{b}&&\ref{definition:MVP}
  \end{align*}

\end{proof}
\end{theorem}

\begin{example}[Matrix notation for systems of linear equations]
  Consider the system of linear equations from \ref{example:NSLE}.

  \begin{align*}
    2x_1+4x_2-3x_3+5x_4+x_5&=9\\
    3x_1+x_2+x_4-3x_5&=0\\
    -2x_1+7x_2-5x_3+2x_4+2x_5&=-3
  \end{align*}
  has coefficient matrix and vector of constants
  \begin{align*}
    A =
    \begin{bmatrix}
      2 & 4 & -3 & 5 & 1\\
      3 & 1 & 0 & 1 & -3\\
      -2 & 7 & -5 & 2 & 2
    \end{bmatrix}
        &\vect{b}&=\colvector{9\\0\\-3}
  \end{align*}
  and so will be described compactly by 
  \begin{multipleChoice}
    \choice[correct]{the vector equation $A\vect{x}=\vect{b}$.}
    \choice{the vector equation $A\vect{b}=\vect{x}$.}
  \end{multipleChoice}

\end{example}

The matrix-vector product is a very natural computation.  We have
motivated it by its connections with systems of equations, but here is
another example.

\begin{example}[Money's best cities]

  Every year \textit{Money} magazine selects several cities in the
  United States as the ``best'' cities to live in, based on a wide
  array of statistics about each city.  This is an example of how the
  editors of \textit{Money} might arrive at a single number that
  consolidates the statistics about a city.  We will analyze Los
  Angeles, Chicago and New York City, based on four criteria: average
  high temperature in July (Farenheit), number of colleges and
  universities in a 30-mile radius, number of toxic waste sites in the
  Superfund environmental clean-up program and a personal crime index
  based on FBI statistics (average = 100, smaller is safer).  It
  should be apparent how to generalize the example to a greater number
  of cities and a greater number of statistics.

  We begin by building a table of statistics.  The rows will be
  labeled with the cities, and the columns with statistical
  categories.  These values are from \textit{Money}'s website in early
  2005.

  \begin{tabular}{||l||c|c|c|c||}\hline
    City & Temp & Colleges & Superfund & Crime\\\hline\hline
    Los Angeles & 77 & 28 & 93 & 254\\\hline
    Chicago & 84 & 38 & 85 & 363\\\hline
    New York & 84 & 99 & 1 & 193\\\hline\hline
  \end{tabular}

  Conceivably these data might reside in a spreadsheet.  Now we must
  combine the statistics for each city.  We could accomplish this by
  weighting each category, scaling the values and summing them.  The
  sizes of the weights would depend upon the numerical size of each
  statistic generally, but more importantly, they would reflect the
  editors opinions or beliefs about which statistics were most
  important to their readers.  Is the crime index more important than
  the number of colleges and universities?  Of course, there is no
  right answer to this question.

  Suppose the editors finally decide on the following weights to
  employ: temperature, $0.23$; colleges, $0.46$; Superfund, $-0.05$;
  crime, $-0.20$.  Notice how negative weights are used for
  undesirable statistics.  Then, for example, the editors would
  compute for Los Angeles,
  \[
    (0.23)(77) + (0.46)(28) + (-0.05)(93) + (-0.20)(254) = -24.86
  \]

  This computation might remind you of an inner product, but we will
  produce the computations for all of the cities as a matrix-vector
  product.  Write the table of raw statistics as a matrix
  \[
    T=
    \begin{bmatrix}
      77 & 28 & 93 & 254\\
      84 & 38 & 85 & 363\\
      84 & 99 & 1 & 193
    \end{bmatrix}
  \]
  and the weights as a vector
  \[
    \vect{w}=\colvector{0.23\\0.46\\-0.05\\-0.20}
  \]
  then the matrix-vector product (\ref{definition:MVP}) yields
  \[
    T\vect{w}=
    (0.23)\colvector{77\\84\\84}+
    (0.46)\colvector{28\\38\\99}+
    (-0.05)\colvector{93\\85\\1}+
    (-0.20)\colvector{254\\363\\193}
    =
    \colvector{-24.86\\-40.05\\26.21}
  \]

  This vector contains a single number for each of the cities being
  studied, so the editors would rank New York best ($26.21$), Los
  Angeles next ($-24.86$), and Chicago third ($-40.05$).  Of course,
  the mayor's offices in Chicago and Los Angeles are free to counter
  with a different set of weights that cause their city to be ranked
  best.  These alternative weights would be chosen to play to each
  cities' strengths, and minimize their problem areas.

  If a speadsheet were used to make these computations, a row of
  weights would be entered somewhere near the table of data and the
  formulas in the spreadsheet would effect a matrix-vector product.
  This example is meant to illustrate how ``linear'' computations
  (addition, multiplication) can be organized as a matrix-vector
  product.

  Another example would be the matrix of numerical scores on
  examinations and exercises for students in a class.  The rows would
  be indexed by students and the columns would be indexed by exams and
  assignments.  The instructor could then assign weights to the
  different exams and assignments, and via a matrix-vector product,
  compute a single score for each student.

\end{example}

Later (much later) we will need the following theorem, which is really
a technical lemma (see \ref{technique:LC}).  Since we are in a
position to prove it now, we will.  But you can safely skip it for the
moment, if you promise to come back later to study the proof when the
theorem is employed.  At that point you will also be able to
understand the comments in the paragraph following the proof.

\begin{theorem}[Equal Matrices and Matrix-Vector Products]
  \label{theorem:EMMVP}
  
  Suppose that $A$ and $B$ are $m\times n$ matrices such that
  $A\vect{x}=B\vect{x}$ for every $\vect{x}\in\complex{n}$.  Then $A=B$.
  
  \begin{proof}
    We are assuming $A\vect{x}=B\vect{x}$ for all $\vect{x}\in\complex{n}$, so we can employ this equality for \textit{any} choice of the vector $\vect{x}$.  However, we will limit our use of this equality to the standard unit vectors, $\vect{e}_j$, $1\leq j\leq n$ (\ref{definition:SUV}).  For all $1\leq j\leq n$, $1\leq i\leq m$,
    \begin{align*}
      &\matrixentry{A}{ij}\\
      &=
        0\matrixentry{A}{i1}+
        \cdots+
        0\matrixentry{A}{i,j-1}+
        1\matrixentry{A}{ij}+
        0\matrixentry{A}{i,j+1}+
        \cdots+
        0\matrixentry{A}{in}
      &&\ref{theorem:PCNA}\\
      &=
        \vectorentry{\vect{e}_j}{1}\matrixentry{A}{i1}+
        \vectorentry{\vect{e}_j}{2}\matrixentry{A}{i2}+
        \vectorentry{\vect{e}_j}{3}\matrixentry{A}{i3}+
        \cdots+
        \vectorentry{\vect{e}_j}{n}\matrixentry{A}{in}
      &&\ref{property:CMCN}\\
      &=
        \matrixentry{A}{i1}\vectorentry{\vect{e}_j}{1}+
        \matrixentry{A}{i2}\vectorentry{\vect{e}_j}{2}+
        \matrixentry{A}{i3}\vectorentry{\vect{e}_j}{3}+
        \cdots+
        \matrixentry{A}{in}\vectorentry{\vect{e}_j}{n}
      &&\ref{definition:SUV}\\
      &=
        \vectorentry{A\vect{e}_j}{i}&&\ref{definition:MVP}\\
      &=
        \vectorentry{B\vect{e}_j}{i}&&\text{Hypothesis}\\
      &=
        \matrixentry{B}{i1}\vectorentry{\vect{e}_j}{1}+
        \matrixentry{B}{i2}\vectorentry{\vect{e}_j}{2}+
        \matrixentry{B}{i3}\vectorentry{\vect{e}_j}{3}+
        \cdots+
        \matrixentry{B}{in}\vectorentry{\vect{e}_j}{n}&&\ref{definition:MVP}\\
      &=
        \vectorentry{\vect{e}_j}{1}\matrixentry{B}{i1}+
        \vectorentry{\vect{e}_j}{2}\matrixentry{B}{i2}+
        \vectorentry{\vect{e}_j}{3}\matrixentry{B}{i3}+
        \cdots+
        \vectorentry{\vect{e}_j}{n}\matrixentry{B}{in}&&\ref{property:CMCN}\\
      &=
        0\matrixentry{B}{i1}+
        \cdots+
        0\matrixentry{B}{i,j-1}+
        1\matrixentry{B}{ij}+
        0\matrixentry{B}{i,j+1}+
        \cdots+
        0\matrixentry{B}{in}
      &&\ref{definition:SUV}\\
      &=\matrixentry{B}{ij}
      &&\ref{theorem:PCNA}
    \end{align*}
    
    So by \ref{definition:ME} the matrices $A$ and $B$ are equal, as desired.

\end{proof}
\end{theorem}

You might notice from studying the proof that the hypotheses of this
theorem could be ``weakened'' (i.e., made less restrictive).  We need
only suppose the equality of the matrix-vector products for just the
standard unit vectors (\ref{definition:SUV}) or any other spanning set
(\ref{definition:SSVS}) of $\complex{n}$.  However, in practice, when
we apply this theorem the stronger hypothesis will be in effect so
this version of the theorem will suffice for our purposes.  (If we
changed the statement of the theorem to have the less restrictive
hypothesis, then we would call the theorem ``stronger.'')

\end{document}
