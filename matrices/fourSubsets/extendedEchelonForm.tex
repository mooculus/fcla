\documentclass{ximera}

\input{../../preamble.tex}

\title{Extended Echelon Form}

\begin{document}
\begin{abstract}
  Computing the column space of a matrix as the null space of an
  associated matrix feels similar to the computation of the inverse of
  a nonsingular matrix.
\end{abstract}
\maketitle

\begin{definition}[Extended Echelon Form]
  Suppose $A$ is an $m\times n$ matrix.  Extend $A$ on its right side
  with the addition of an $m\times m$ identity matrix to form an
  $m\times (n + m)$ matrix $M$.  Use row operations to bring $M$ to
  reduced row-echelon form and call the result $N$.  $N$ is the
  \dfn{extended reduced row-echelon form} of $A$, and we will
  standardize on names for five submatrices ($B$, $C$, $J$, $K$, $L$)
  of $N$.

  Let $B$ denote the $m\times n$ matrix formed from the first $n$
  columns of $N$ and let $J$ denote the $m\times m$ matrix formed from
  the last $m$ columns of $N$.  Suppose that $B$ has $r$ nonzero rows.
  Further partition $N$ by letting $C$ denote the $r\times n$ matrix
  formed from all of the nonzero rows of $B$.  Let $K$ be the
  $r\times m$ matrix formed from the first $r$ rows of $J$, while $L$
  will be the $(m-r)\times m$ matrix formed from the bottom $m-r$ rows
  of $J$.  Pictorially,
  \[
    M=[A\vert I_m]
    \rref
    N=[B\vert J]
    =
    \left[\begin{array}{c|c}C&K\\\hline0&L\end{array}\right]
  \]
\end{definition}

\begin{example}[Submatrices of extended echelon form]

  We illustrate extended echelon form with the matrix $A$,
  \[
    A=
    \begin{bmatrix}
      1 & -1 & -2 & 7 & 1 & 6 \\
      -6 & 2 & -4 & -18 & -3 & -26 \\
      4 & -1 & 4 & 10 & 2 & 17 \\
      3 & -1 & 2 & 9 & 1 & 12
    \end{bmatrix}
  \]

  Augmenting with the $4\times 4$ identity matrix,
  \[
    M=
    \begin{bmatrix}
      1 & -1 & -2 & 7 & 1 & 6 & 1 & 0 & 0 & 0 \\
      -6 & 2 & -4 & -18 & -3 & -26 & 0 & 1 & 0 & 0 \\
      4 & -1 & 4 & 10 & 2 & 17 & 0 & 0 & 1 & 0 \\
      3 & -1 & 2 & 9 & 1 & 12 & 0 & 0 & 0 & 1
    \end{bmatrix}
  \]
  and row-reducing, we obtain
  \[
    N=
    \begin{bmatrix}
      \leading{1} & 0 & 2 & 1 & 0 & 3 & 0 & 1 & 1 & 1\\
      0 & \leading{1} & 4 & -6 & 0 & -1 & 0 & 2 & 3 & 0 \\
      0 & 0 & 0 & 0 & \leading{1} & 2 & 0 & -1 & 0 & -2 \\
      0 & 0 & 0 & 0 & 0 & 0 & \leading{1} & 2 & 2 & 1
    \end{bmatrix}
  \]
  
  So we then obtain
  \begin{align*}
    B&=
       \begin{bmatrix}
         \leading{1} & 0 & 2 & 1 & 0 & 3 \\
         0 & \leading{1} & 4 & -6 & 0 & -1 \\
         0 & 0 & 0 & 0 & \leading{1} & 2 \\
         0 & 0 & 0 & 0 & 0 & 0
       \end{bmatrix}\\
    C&=
       \begin{bmatrix}
         \leading{1} & 0 & 2 & 1 & 0 & 3 \\
         0 & \leading{1} & 4 & -6 & 0 & -1 \\
         0 & 0 & 0 & 0 & \leading{1} & 2
       \end{bmatrix}\\
    J&=
       \begin{bmatrix}
         0 & 1 & 1 & 1\\
         0 & 2 & 3 & 0 \\
         0 & -1 & 0 & -2 \\
         \leading{1} & 2 & 2 & 1
       \end{bmatrix}\\
    K&=
       \begin{bmatrix}
         0 & 1 & 1 & 1\\
         0 & 2 & 3 & 0 \\
         0 & -1 & 0 & -2
       \end{bmatrix}\\
    L&=
       \begin{bmatrix}
         \leading{1} & \answer{2} & 2 & 1
       \end{bmatrix}
  \end{align*}
\end{example}

You can observe (or verify) the properties of the following theorem
with the preceeding example.

\begin{theorem}[Properties of Extended Echelon Form]
  \label{theorem:PEEF}

  Suppose that $A$ is an $m\times n$ matrix and that $N$ is its extended echelon form.  Then
  \begin{enumerate}
  \item $J$ is nonsingular.
  \item $B=JA$.
  \item If $\vect{x}\in\complex{n}$ and $\vect{y}\in\complex{m}$, then $A\vect{x}=\vect{y}$ if and only if $B\vect{x}=J\vect{y}$.
  \item $C$ is in reduced row-echelon form, has no zero rows and has $r$ pivot columns.
  \item $L$ is in reduced row-echelon form, has no zero rows and has $m-r$ pivot columns.
  \end{enumerate}

  \begin{proof}
    $J$ is the result of applying a sequence of row operations to
    $I_m$, and therefore $J$ and $I_m$ are row-equivalent.
    $\homosystem{I_m}$ has only the zero solution, since $I_m$ is
    nonsingular (\ref{theorem:NMRRI}).  Thus, $\homosystem{J}$ also
    has only the zero solution (\ref{theorem:REMES},
    \ref{definition:ESYS}) and $J$ is therefore nonsingular
    (\ref{definition:NSM}).

    To prove the second part of this conclusion, first convince
    yourself that row operations and the matrix-vector product are
    associative operations.  By this we mean the following.  Suppose
    that $F$ is an $m\times n$ matrix that is row-equivalent to the
    matrix $G$.  Apply to the column vector $F\vect{w}$ the same
    sequence of row operations that converts $F$ to $G$.  Then the
    result is 
    \begin{multipleChoice}
      \choice[correct]{$G\vect{w}$.}
      \choice{$F\vect{w}$.}
    \end{multipleChoice}

    So we can do row operations on the matrix, then do a matrix-vector
    product, \textit{or} do a matrix-vector product and then do row
    operations on a column vector, and the result will be the same
    either way.  Since matrix multiplication is defined by a
    collection of matrix-vector products (\ref{definition:MM}), the
    matrix product $FH$ will become $GH$ if we apply the same sequence
    of row operations to $FH$ that convert $F$ to $G$.  Now apply
    these observations to $A$.

    Write $AI_n=I_mA$ and apply the row operations that convert $M$ to
    $N$.  $A$ is converted to $B$, while $I_m$ is converted to $J$, so
    we have $BI_n=JA$.  Simplifying the left side gives the desired
    conclusion.

    For the third conclusion, we now establish the two equivalences
    \begin{align*}
      A\vect{x}&=\vect{y} &
      &\iff &
              JA\vect{x}&=J\vect{y} &
      &\iff &
              B\vect{x}&=J\vect{y}
    \end{align*}

    The forward direction of the first equivalence is accomplished by
    multiplying both sides of the matrix equality by $J$, while the
    backward direction is accomplished by multiplying by the inverse
    of $J$ (which we know exists by \ref{theorem:NI} since $J$ is
    nonsingular).  The second equivalence is obtained simply by the
    substitutions given by $JA=B$.

    The first $r$ rows of $N$ are in reduced row-echelon form, since
    any contiguous collection of rows taken from a matrix in reduced
    row-echelon form will form a matrix that is again in reduced
    row-echelon form.  Since the matrix $C$ is formed by removing the
    last $n$ entries of each these rows, the remainder is still in
    reduced row-echelon form.  By its construction, $C$ has no zero
    rows. $C$ has $r$ rows and each contains a leading 1, so there are
    $r$ pivot columns in $C$.

    The final $m-r$ rows of $N$ are in reduced row-echelon form, since
    any contiguous collection of rows taken from a matrix in reduced
    row-echelon form will form a matrix that is again in reduced
    row-echelon form.  Since the matrix $L$ is formed by removing the
    first $n$ entries of each these rows, and these entries are all
    zero (they form the zero rows of $B$), the remainder is still in
    reduced row-echelon form.  $L$ is the final $m-r$ rows of the
    nonsingular matrix $J$, so none of these rows can be totally zero,
    or $J$ would not row-reduce to the identity matrix.  $L$ has $m-r$
    rows and each contains a leading 1, so there are $\answer{m-r}$
    pivot columns in $L$.
\end{proof}
\end{theorem}

Notice that in the case where $A$ is a nonsingular matrix we know that
the reduced row-echelon form of $A$ is the identity matrix
(\ref{theorem:NMRRI}), so $B=I_n$.  Then the second conclusion above
says $JA=B=I_n$, so $J$ is the inverse of $A$.  Thus this theorem
generalizes \ref{theorem:CINM}, though the result is a
``left-inverse'' of $A$ rather than a ``right-inverse.''

The third conclusion of \ref{theorem:PEEF} is the most telling.  It
says that $\vect{x}$ is a solution to the linear system
$\linearsystem{A}{\vect{y}}$ if and only if $\vect{x}$ is a solution
to the linear system $\linearsystem{B}{J\vect{y}}$.  Or said
differently, if we row-reduce the augmented matrix
$\augmented{A}{\vect{y}}$ we will get the augmented matrix
$\augmented{B}{J\vect{y}}$.  The matrix $J$ tracks the cumulative
effect of the row operations that converts $A$ to reduced row-echelon
form, here effectively applying them to the vector of constants in a
system of equations having $A$ as a coefficient matrix.  When $A$
row-reduces to a matrix with zero rows, then $J\vect{y}$ should also
have zero entries in the same rows if the system is to be consistent.

\end{document}
