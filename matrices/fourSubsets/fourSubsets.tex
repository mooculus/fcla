\documentclass{ximera}

\input{../../preamble.tex}

\title{Four Subsets}

\begin{document}
\begin{abstract}
  We can find linearly independent sets to describe the null space,
  column space, row space, left null space by analyzing only the
  extended echelon form of the matrix.
\end{abstract}
\maketitle

With all the preliminaries in place we can state a theorem. In
essence, this result will allow us to say that we can find linearly
independent sets to use in span constructions for all four subsets
(null space, column space, row space, left null space) by analyzing
only the extended echelon form of the matrix, and specifically, just
the two submatrices $C$ and $L$, which will be ripe for analysis since
they are already in reduced row-echelon form (\ref{theorem:PEEF}).

\begin{theorem}[Four Subsets]
\label{theorem:FS}
Suppose $A$ is an $m\times n$ matrix with extended echelon form $N$.
Suppose the reduced row-echelon form of $A$ has $r$ nonzero rows.
Then $C$ is the submatrix of $N$ formed from the first $r$ rows and
the first $n$ columns and $L$ is the submatrix of $N$ formed from the
last $m$ columns and the last $m-r$ rows.  Then
\begin{enumerate}
\item The null space of $A$ is the null space of $C$, $\nsp{A}=\nsp{C}$.
\item The row space of $A$ is the row space of $C$, $\rsp{A}=\rsp{C}$.
\item The column space of $A$ is the null space of $L$, $\csp{A}=\nsp{L}$.
\item The left null space of $A$ is the row space of $L$, $\lns{A}=\rsp{L}$.
\end{enumerate}

\begin{proof}
  First, $\nsp{A}=\nsp{B}$ since $B$ is row-equivalent to $A$
  (\ref{theorem:REMES}).  The zero rows of $B$ represent equations
  that are always true in the homogeneous system $\homosystem{B}$, so
  the removal of these equations will not change the solution set.
  Thus, in turn, $\nsp{B}=\nsp{C}$.

  Second, $\rsp{A}=\rsp{B}$ since $B$ is row-equivalent to $A$
  (\ref{theorem:REMRS}).  The zero rows of $B$ contribute nothing to
  the span that is the row space of $B$, so the removal of these rows
  will not change the row space.  Thus, in turn, $\rsp{B}=\rsp{C}$.

  Third, we prove the set equality $\csp{A}=\nsp{L}$ with
  \ref{definition:SE}.  Begin by showing that
  $\csp{A}\subseteq\nsp{L}$.  Choose
  $\vect{y}\in\csp{A}\subseteq\complex{m}$.  Then there exists a
  vector $\vect{x}\in\complex{n}$ such that $A\vect{x}=\vect{y}$
  (\ref{theorem:CSCS}).  Then for $1\leq k\leq m-r$,
  \begin{align*}
    \vectorentry{L\vect{y}}{k}
    &=\vectorentry{J\vect{y}}{r+k}
    &&\text{$L$ a submatrix of $J$}\\
    &=\vectorentry{B\vect{x}}{r+k}
    &&\ref{theorem:PEEF}\\
    &=\vectorentry{\zeromatrix\vect{x}}{k}
    &&\text{Zero matrix a submatrix of $B$}\\
    &=\vectorentry{\zerovector}{k}
    &&\ref{theorem:MMZM}
  \end{align*}
  
  So, for all $1\leq k\leq m-r$,
  $\vectorentry{L\vect{y}}{k}=\vectorentry{\zerovector}{k}$.  So by
  \ref{definition:CVE} we have $L\vect{y}=\zerovector$ and thus
  $\vect{y}\in\nsp{L}$.

  Now, show that $\nsp{L}\subseteq\csp{A}$.  Choose
  $\vect{y}\in\nsp{L}\subseteq\complex{m}$.  Form the vector
  $K\vect{y}\in\complex{r}$.  The linear system
  $\linearsystem{C}{K\vect{y}}$ is consistent since $C$ is in reduced
  row-echelon form and has no zero rows (\ref{theorem:PEEF}).  Let
  $\vect{x}\in\complex{n}$ denote a solution to
  $\linearsystem{C}{K\vect{y}}$.

  Then for $1\leq j\leq r$,
  \begin{align*}
    \vectorentry{B\vect{x}}{j}
    &=\vectorentry{C\vect{x}}{j}
    &&\text{$C$ a submatrix of $B$}\\
    &=\vectorentry{K\vect{y}}{j}
    &&\text{$\vect{x}$ a solution to $\linearsystem{C}{K\vect{y}}$}\\
    &=\vectorentry{J\vect{y}}{j}
    &&\text{$K$ a submatrix of $J$}\\
  \end{align*}
  
  And for $r+1\leq k\leq m$,
  \begin{align*}
    \vectorentry{B\vect{x}}{k}
    &=\vectorentry{\zeromatrix\vect{x}}{k-r}
    &&\text{Zero matrix a submatrix of $B$}\\
    &=\vectorentry{\zerovector}{k-r}
    &&\ref{theorem:MMZM}\\
    &=\vectorentry{L\vect{y}}{k-r}
    &&\text{$\vect{y}$ in $\nsp{L}$}\\
    &=\vectorentry{J\vect{y}}{k}
    &&\text{$L$ a submatrix of $J$}\\
  \end{align*}
  
  So for all $1\leq i\leq m$,
  $\vectorentry{B\vect{x}}{i}=\vectorentry{J\vect{y}}{i}$ and by
  \ref{definition:CVE} we have $B\vect{x}=J\vect{y}$.  From
  \ref{theorem:PEEF} we know then that $A\vect{x}=\vect{y}$, and
  therefore $\vect{y}\in\csp{A}$ (\ref{theorem:CSCS}).  By
  \ref{definition:SE} we now have $\csp{A}=\nsp{L}$.

  Fourth, we prove the set equality $\lns{A}=\rsp{L}$ with
  \ref{definition:SE}.  Begin by showing that
  $\rsp{L}\subseteq\lns{A}$.  Choose
  $\vect{y}\in\rsp{L}\subseteq\complex{m}$.  Then there exists a
  vector $\vect{w}\in\complex{m-r}$ such that
  $\vect{y}=\transpose{L}\vect{w}$ (\ref{definition:RSM},
  \ref{theorem:CSCS}).  Then for $1\leq i\leq n$,
  \begin{align*}
    \vectorentry{\transpose{A}\vect{y}}{i}
    &=\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\vectorentry{\vect{y}}{k}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\vectorentry{\transpose{L}\vect{w}}{k}
    &&\text{Definition of $\vect{w}$}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\sum_{\ell=1}^{m-r}\matrixentry{\transpose{L}}{k\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{m}\sum_{\ell=1}^{m-r}\matrixentry{\transpose{A}}{ik}\matrixentry{\transpose{L}}{k\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{property:DCN}\\
    &=\sum_{\ell=1}^{m-r}\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\matrixentry{\transpose{L}}{k\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{property:CACN}\\
    &=\sum_{\ell=1}^{m-r}\left(\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\matrixentry{\transpose{L}}{k\ell}\right)\vectorentry{\vect{w}}{\ell}
    &&\ref{property:DCN}\\
    &=\sum_{\ell=1}^{m-r}\left(\sum_{k=1}^{m}\matrixentry{\transpose{A}}{ik}\matrixentry{\transpose{J}}{k,r+\ell}\right)\vectorentry{\vect{w}}{\ell}
    &&\text{$L$ a submatrix of $J$}\\
    &=\sum_{\ell=1}^{m-r}\matrixentry{\transpose{A}\transpose{J}}{i,r+\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{theorem:EMP}\\
    &=\sum_{\ell=1}^{m-r}\matrixentry{\transpose{\left(JA\right)}}{i,r+\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{theorem:MMT}\\
    &=\sum_{\ell=1}^{m-r}\matrixentry{\transpose{B}}{i,r+\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{theorem:PEEF}\\
    &=\sum_{\ell=1}^{m-r}0\vectorentry{\vect{w}}{\ell}
    &&\text{Zero rows in $B$}\\
    &=0
    &&\ref{property:ZCN}\\
    &=\vectorentry{\zerovector}{i}
    &&\ref{definition:ZCV}
  \end{align*}
  
  Since
  $\vectorentry{\transpose{A}\vect{y}}{i}=\vectorentry{\zerovector}{i}$
  for $1\leq i\leq n$, \ref{definition:CVE} implies that
  $\transpose{A}\vect{y}=\zerovector$.  This means that
  $\vect{y}\in\nsp{\transpose{A}}$.

  Now, show that $\lns{A}\subseteq\rsp{L}$.  Choose
  $\vect{y}\in\lns{A}\subseteq\complex{m}$.  The matrix $J$ is
  nonsingular (\ref{theorem:PEEF}), so $\transpose{J}$ is also
  nonsingular (\ref{theorem:MIT}) and therefore the linear system
  $\linearsystem{\transpose{J}}{\vect{y}}$ has a unique solution.
  Denote this solution as $\vect{x}\in\complex{m}$.  We will need to
  work with two ``halves'' of $\vect{x}$, which we will denote as
  $\vect{z}$ and $\vect{w}$ with formal definitions given by
  \begin{align*}
    \vectorentry{z}{j}&=\vectorentry{x}{i}
    &
    &1\leq j\leq r,
    &&&
        \vectorentry{w}{k}&=\vectorentry{x}{r+k}
    &
    &1\leq k\leq m-r
  \end{align*}

  Now, for $1\leq j\leq r$,
  \begin{align*}
    \vectorentry{\transpose{C}\vect{z}}{j}
    &=\sum_{k=1}^{r}\matrixentry{\transpose{C}}{jk}\vectorentry{\vect{z}}{k}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{C}}{jk}\vectorentry{\vect{z}}{k}+
      \sum_{\ell=1}^{m-r}\matrixentry{\zeromatrix}{j\ell}\vectorentry{\vect{w}}{\ell}
    &&\ref{definition:ZM}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{B}}{jk}\vectorentry{\vect{z}}{k}+
      \sum_{\ell=1}^{m-r}\matrixentry{\transpose{B}}{j,r+\ell}\vectorentry{\vect{w}}{\ell}
    &&\text{$C$, $\zeromatrix$ submatrices of $B$}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{B}}{jk}\vectorentry{\vect{x}}{k}+
      \sum_{\ell=1}^{m-r}\matrixentry{\transpose{B}}{j,r+\ell}\vectorentry{\vect{x}}{r+\ell}
    &&\text{Definitions of $\vect{z}$ and $\vect{w}$}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{B}}{jk}\vectorentry{\vect{x}}{k}+
      \sum_{k=r+1}^{m}\matrixentry{\transpose{B}}{jk}\vectorentry{\vect{x}}{k}
    &&\text{Re-index second sum}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{B}}{jk}\vectorentry{\vect{x}}{k}
    &&\text{Combine sums}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{\left(JA\right)}}{jk}\vectorentry{\vect{x}}{k}
    &&\ref{theorem:PEEF}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{A}\transpose{J}}{jk}\vectorentry{\vect{x}}{k}
    &&\ref{theorem:MMT}\\
    &=\sum_{k=1}^{m}\sum_{\ell=1}^{m}\matrixentry{\transpose{A}}{j\ell}\matrixentry{\transpose{J}}{\ell k}\vectorentry{\vect{x}}{k}
    &&\ref{theorem:EMP}\\
    &=\sum_{\ell=1}^{m}\sum_{k=1}^{m}\matrixentry{\transpose{A}}{j\ell}\matrixentry{\transpose{J}}{\ell k}\vectorentry{\vect{x}}{k}
    &&\ref{property:CACN}\\
    &=\sum_{\ell=1}^{m}\matrixentry{\transpose{A}}{j\ell}\left(\sum_{k=1}^{m}\matrixentry{\transpose{J}}{\ell k}\vectorentry{\vect{x}}{k}\right)
    &&\ref{property:DCN}\\
    &=\sum_{\ell=1}^{m}\matrixentry{\transpose{A}}{j\ell}\vectorentry{\transpose{J}\vect{x}}{\ell}
    &&\ref{theorem:EMP}\\
    &=\sum_{\ell=1}^{m}\matrixentry{\transpose{A}}{j\ell}\vectorentry{\vect{y}}{\ell}
    &&\text{Definition of $\vect{x}$}\\
    &=\vectorentry{\transpose{A}\vect{y}}{j}
    &&\ref{theorem:EMP}\\
    &=\vectorentry{\zerovector}{j}
    &&\text{$\vect{y}\in\lns{A}$}
  \end{align*}
  
  So, by \ref{definition:CVE}, $\transpose{C}\vect{z}=\zerovector$ and
  the vector $\vect{z}$ gives us a linear combination of the columns
  of $\transpose{C}$ that equals the zero vector.  In other words,
  $\vect{z}$ gives a relation of linear dependence on the the rows of
  $C$.  However, the rows of $C$ are a linearly independent set by
  \ref{theorem:BRS}.  According to \ref{definition:LICV} we must
  conclude that the entries of $\vect{z}$ are all zero, i.e.,
  $\vect{z}=\zerovector$.

  Now, for $1\leq i\leq m$, we have
  \begin{align*}
    \vectorentry{\vect{y}}{i}
    &=\vectorentry{\transpose{J}\vect{x}}{i}
    &&\text{Definition of $\vect{x}$}\\
    &=\sum_{k=1}^{m}\matrixentry{\transpose{J}}{ik}\vectorentry{\vect{x}}{k}
    &&\ref{theorem:EMP}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{J}}{ik}\vectorentry{\vect{x}}{k}+
      \sum_{k=r+1}^{m}\matrixentry{\transpose{J}}{ik}\vectorentry{\vect{x}}{k}
    &&\text{Break apart sum}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{J}}{ik}\vectorentry{\vect{z}}{k}+
      \sum_{k=r+1}^{m}\matrixentry{\transpose{J}}{ik}\vectorentry{\vect{w}}{k-r}
    &&\text{Definition of $\vect{z}$ and $\vect{w}$}\\
    &=\sum_{k=1}^{r}\matrixentry{\transpose{J}}{ik}0+
      \sum_{\ell=1}^{m-r}\matrixentry{\transpose{J}}{i,r+\ell}\vectorentry{\vect{w}}{\ell}
    &&\text{$\vect{z}=\zerovector$, re-index}\\
    &=0+\sum_{\ell=1}^{m-r}\matrixentry{\transpose{L}}{i,\ell}\vectorentry{\vect{w}}{\ell}
    &&\text{$L$ a submatrix of $J$}\\
    &=\vectorentry{\transpose{L}\vect{w}}{i}
    &&\ref{theorem:EMP}\\
  \end{align*}

  So by \ref{definition:CVE}, $\vect{y}=\transpose{L}\vect{w}$.  The
  existence of $\vect{w}$ implies that $\vect{y}\in\rsp{L}$, and
  therefore $\lns{A}\subseteq\rsp{L}$.  So by \ref{definition:SE} we
  have $\lns{A}=\rsp{L}$.

\end{proof}
\end{theorem}

The first two conclusions of this theorem are nearly trivial.  But
they set up a pattern of results for $C$ that is reflected in the
latter two conclusions about $L$.  In total, they tell us that we can
compute all four subsets just by finding null spaces and row spaces.
This theorem does not tell us exactly how to compute these subsets,
but instead simply expresses them as null spaces and row spaces of
matrices in reduced row-echelon form without any zero rows ($C$ and
$L$).  A linearly independent set that spans the null space of a
matrix in reduced row-echelon form can be found easily with
\ref{theorem:BNS}.  It is an even easier matter to find a linearly
independent set that spans the row space of a matrix in reduced
row-echelon form with \ref{theorem:BRS}, especially when there are no
zero rows present.  So an application of \ref{theorem:FS} is typically
followed by two applications each of \ref{theorem:BNS} and
\ref{theorem:BRS}.

The situation when $r=m$ deserves comment, since now the matrix $L$
has no rows.  What is $\csp{A}$ when we try to apply \ref{theorem:FS}
and encounter $\nsp{L}$?  One interpretation of this situation is that
$L$ is the coefficient matrix of a homogeneous system that has no
equations.  How hard is it to find a solution vector to this system?
Some thought will convince you that \textit{any} proposed vector will
qualify as a solution, since it makes \textit{all} of the equations
true.  So every possible vector is in the null space of $L$ and
therefore $\csp{A}=\nsp{L}=\complex{m}$.  Perhaps you are not
convinced?  Let us try another argument that might solidly convince
you of this logic.

If $r=m$, when we row-reduce the augmented matrix of
$\linearsystem{A}{\vect{b}}$ the result will have no zero rows, and
the first $n$ columns will all be pivot columns, leaving none for the
final column, so by \ref{theorem:RCLS} the system will be consistent.
By \ref{theorem:CSCS}, $\vect{b}\in\csp{A}$.  Since $\vect{b}$ was
arbitrary, every possible vector is in the column space of $A$, so we
again have $\csp{A}=\complex{m}$.  The situation when a matrix has
$r=m$ is known by the term \dfn{full rank}, and in the case of a
square matrix coincides with nonsingularity.

The properties of the matrix $L$ described by this theorem can be
explained informally as follows.  A column vector
$\vect{y}\in\complex{m}$ is in the column space of $A$ if the linear
system $\linearsystem{A}{\vect{y}}$ is consistent
(\ref{theorem:CSCS}).  By \ref{theorem:RCLS}, the reduced row-echelon
form of the augmented matrix $\augmented{A}{\vect{y}}$ of a consistent
system will have zeros in the bottom $m-r$ locations of the last
column.  By \ref{theorem:PEEF} this final column is the vector
$J\vect{y}$ and so should then have zeros in the final $m-r$
locations.  But since $L$ comprises the final $m-r$ rows of $J$, this
condition is expressed by saying $\vect{y}\in\nsp{L}$.

Additionally, the rows of $J$ are the scalars in linear combinations
of the rows of $A$ that create the rows of $B$.  That is, the rows of
$J$ record the net effect of the sequence of row operations that takes
$A$ to its reduced row-echelon form, $B$.  This can be seen in the
equation $JA=B$ (\ref{theorem:PEEF}).  As such, the rows of $L$ are
scalars for linear combinations of the rows of $A$ that yield zero
rows.  But such linear combinations are precisely the elements of the
left null space.  So any element of the row space of $L$ is also an
element of the left null space of $A$.

We will now illustrate \ref{theorem:FS} with a few examples.

\begin{example}[Four subsets, no. 1]

  In \ref{example:SEEF} we found the five relevant submatrices of the
  extended echelon form for the matrix
  \[
    A=
    \begin{bmatrix}
      1 & -1 & -2 & 7 & 1 & 6 \\
      -6 & 2 & -4 & -18 & -3 & -26 \\
      4 & -1 & 4 & 10 & 2 & 17 \\
      3 & -1 & 2 & 9 & 1 & 12
    \end{bmatrix}
  \]
  
  To apply \ref{theorem:FS} we only need $C$ and $L$,
  \begin{align*}
    C&=
       \begin{bmatrix}
         \leading{1} &                0 & 2 & 1 &                0 & 3 \\
         0 & \leading{1} & 4 & -6 &               0 & -1 \\
         0 &                0 & 0 & 0 & \leading{1} & 2
       \end{bmatrix}
                                                                   &
                                                                     L&=
                                                                        \begin{bmatrix}
                                                                          \leading{1} & 2 & 2 & 1
                                                                        \end{bmatrix}
  \end{align*}
  
  Then we use \ref{theorem:FS}  to obtain
  \begin{align*}
    \nsp{A}&=\nsp{C}=
             \spn{\set{
             \colvector{-2\\-4\\1\\0\\0\\0},\,
    \colvector{-1\\6\\0\\1\\0\\0},\,
    \colvector{-3\\1\\0\\0\\-2\\1}
    }}
           &&\ref{theorem:BNS}\\
    \rsp{A}&=\rsp{C}=
             \spn{\set{
             \colvector{1\\0\\2\\1\\0\\3 },\,
    \colvector{0\\1\\4\\-6\\0\\-1},\,
    \colvector{0\\0\\0\\0\\1\\2}
    }}
           &&\ref{theorem:BRS}\\
    \csp{A}&=\nsp{L}=
             \spn{\set{
             \colvector{-2\\1\\0\\0},\,
    \colvector{-2\\0\\1\\0},\,
    \colvector{-1\\0\\0\\1}
    }}
           &&\ref{theorem:BNS}\\
    \lns{A}&=\rsp{L}=
             \spn{\set{
             \colvector{1\\2\\2\\1}
    }}
           &&\ref{theorem:BRS}
  \end{align*}

  Boom!
\end{example}

\begin{example}[Four subsets, no. 2]

  Now let us return to the matrix $A$ that we used to motivate this section in \ref{example:CSANS},
  \[
    A=
    \begin{bmatrix}
      10 & 0 & 3 & 8 & 7 \\
      -16 & -1 & -4 & -10 & -13 \\
      -6 & 1 & -3 & -6 & -6 \\
      0 & 2 & -2 & -3 & -2 \\
      3 & 0 & 1 & 2 & 3 \\
      -1 & -1 & 1 & 1 & 0
    \end{bmatrix}
  \]

  We form the matrix $M$ by adjoining the $6\times 6$ identity matrix $I_6$,
  \[
    M=
    \begin{bmatrix}
      10 & 0 & 3 & 8 & 7 & 1 & 0 & 0 & 0 & 0 & 0 \\
      -16 & -1 & -4 & -10 & -13 & 0 & 1 & 0 & 0 & 0 & 0 \\
      -6 & 1 & -3 & -6 & -6 & 0 & 0 & 1 & 0 & 0 & 0 \\
      0 & 2 & -2 & -3 & -2 & 0 & 0 & 0 & 1 & 0 & 0 \\
      3 & 0 & 1 & 2 & 3 & 0 & 0 & 0 & 0 & 1 & 0 \\
      -1 & -1 & 1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 1
    \end{bmatrix}
  \]
  and row-reduce to obtain $N$
  \[
    N=
    \begin{bmatrix}
      \leading{1} & 0 & 0 & 0 & 2 & 0 & 0 & 1 & -1 & 2 & -1 \\
      0 & \leading{1} & 0 & 0 & -3 & 0 & 0 & -2 & 3 & -3 & 3 \\
      0 & 0 & \leading{1} & 0 & 1 & 0 & 0 & 1 & 1 & 3 & 3 \\
      0 & 0 & 0 & \leading{1} & -2 & 0 & 0 & -2 & 1 & -4 & 0 \\
      0 & 0 & 0 & 0 & 0 & \leading{1} & 0 & 3 & -1 & 3 & 1 \\
      0 & 0 & 0 & 0 & 0 & 0 & \leading{1} & -2 & 1 & 1 & -1
    \end{bmatrix}
  \]
  
  To find the four subsets for $A$, we only need identify the
  $4\times 5$ matrix $C$ and the $2\times 6$ matrix $L$,
  \begin{align*}
    C&=
       \begin{bmatrix}
         \leading{1} & 0 & 0 & 0 & 2\\
         0 & \leading{1} & 0 & 0 & -3\\
         0 & 0 & \leading{1} & 0 & 1\\
         0 & 0 & 0 & \leading{1} & -2
       \end{bmatrix}
                                 &
                                   L&=
                                      \begin{bmatrix}
                                        \leading{1} & 0 & 3 & -1 & 3 & 1 \\
                                        0 & \leading{1} & -2 & 1 & 1 & -1
                                      \end{bmatrix}
  \end{align*}

  Then we apply \ref{theorem:FS},
  \begin{align*}
    \nsp{A}&=\nsp{C}=
             \spn{\set{
             \colvector{-2\\3\\-1\\2\\1}
    }}
           &&\ref{theorem:BNS}\\
    \rsp{A}&=\rsp{C}=
             \spn{\set{
             \colvector{1 \\ 0 \\ 0 \\ 0 \\ 2},\,
    \colvector{0 \\ 1 \\ 0 \\ 0 \\ -3},\,
    \colvector{0 \\ 0 \\ 1 \\ 0 \\ 1},\,
    \colvector{0 \\ 0 \\ 0 \\ 1 \\ -2}
    }}
           &&\ref{theorem:BRS}\\
    \csp{A}&=\nsp{L}=
             \spn{\set{
             \colvector{-3\\2\\1\\0\\0\\0},\,
    \colvector{1\\-1\\0\\1\\0\\0},\,
    \colvector{-3\\-1\\0\\0\\1\\0},\,
    \colvector{-1\\1\\0\\0\\0\\1}
    }}
           &&\ref{theorem:BNS}\\
    \lns{A}&=\rsp{L}=
             \spn{\set{
             \colvector{1 \\ 0 \\ 3 \\ -1 \\ 3 \\ 1},\,
    \colvector{0 \\ 1 \\ -2 \\ 1 \\ 1 \\ -1}
    }}
           &&\ref{theorem:BRS}
  \end{align*}
\end{example}

The next example is just a bit different since the matrix has more
rows than columns, and a trivial null space.

\begin{example}[Four subsets]
  Consider a system of $m=5$ equations in $n=2$ variables with coefficient matrix
  \[
    G=
    \begin{bmatrix}
      2 & 3\\
      -1 & 4 \\
      3 & 10\\
      3 &  -1\\
      6 & 9
    \end{bmatrix}
  \]

  Adjoin the $5\times 5$ identity matrix, $I_5$, to form
  \[
    M=
    \begin{bmatrix}
      2 & 3 & 1&0&0&0&0\\
      -1 & 4 & 0&1&0&0&0\\
      3 & 10 & 0&0&1&0&0\\
      3 &  -1 & 0&0&0&1&\answer{0}\\
      6 & 9 & 0&0&0&0&\answer{1}
    \end{bmatrix}
  \]

  This row-reduces to
  \[
    N=
    \begin{bmatrix}
      \leading{1} & 0 & 0 & 0 & 0 & \frac{3}{11} & \frac{1}{33}\\
      0 & \leading{1} & 0 & 0 & 0 & -\frac{2}{11} & \frac{1}{11}\\
      0 & 0 & \leading{1} & 0 & 0 & 0 & -\frac{1}{3}\\
      0 & 0 & 0 & \leading{1} & 0 & 1 & -\frac{1}{3}\\
      0 & 0 & 0 & 0 & \leading{1} & 1 & -1
    \end{bmatrix}
  \]

  The first $n=2$ columns contain $r=\answer{2}$ leading 1's, so we
  obtain $C$ as the $2\times 2$ identity matrix and extract $L$ from
  the final $m-r=3$ rows in the final $m=5$ columns.
  \begin{align*}
    C&=
       \begin{bmatrix}
         \leading{1} & 0\\
         0 & \leading{1}
       \end{bmatrix}
                     &
                       L&=
                          \begin{bmatrix}
                            \leading{1} & 0 & 0 & 0 & -\frac{1}{3}\\
                            0 & \leading{1}  & 0 & 1 & -\frac{1}{3}\\
                            0 & 0 & \leading{1}  & 1 & -1
                          \end{bmatrix}
  \end{align*}
  
  Then we apply \ref{theorem:FS},
  \begin{align*}
    \nsp{G}=\nsp{C}&=
                     \spn{\emptyset}
                     =\set{\zerovector}
    &&\ref{theorem:BNS}\\
    \rsp{G}=\rsp{C}&=
                     \spn{\set{
                     \colvector{1 \\ 0},\,
    \colvector{0 \\ 1}
    }}
    =\complex{2}
                   &&\ref{theorem:BRS}\\
    \csp{G}=\nsp{L}&=
                     \spn{\set{
                     \colvector{0\\-1\\-1\\1\\0},\,
    \colvector{\frac{1}{3}\\\frac{1}{3}\\1\\0\\1}
    }}&&\ref{theorem:BNS}\\
                   &=\spn{\set{
                     \colvector{0\\-1\\-1\\1\\0},\,
    \colvector{1\\1\\3\\0\\3}
    }}\\
    \lns{G}=\rsp{L}&=
                     \spn{\set{
                     \colvector{1 \\ 0 \\ 0 \\ 0 \\ -\frac{1}{3}},\,
    \colvector{0 \\ 1  \\ 0 \\ 1 \\ -\frac{1}{3}},\,
    \colvector{0 \\ 0 \\ 1  \\ 1 \\ -1}
    }}&&\ref{theorem:BRS}\\
                   &=\spn{\set{
                     \colvector{3 \\ 0 \\ 0 \\ 0 \\ -1},\,
    \colvector{0 \\ 3  \\ 0 \\ 3\ \\ -1},\,
    \colvector{0 \\ 0 \\ 1  \\ 1 \\ -1}
    }}
  \end{align*}

  Depending on the choice of $\vec{b}$, the linear system
  $\linearsystem{G}{\vec{b}}$ may or may not be consistent.  See if
  you can write the two different vectors of constants from these two
  archetypes as linear combinations of the two vectors that form the
  spanning set for $\csp{G}$.  How about the two columns of $G$?  Can
  you write each individually as a linear combination of the two
  vectors that form the spanning set for $\csp{G}$?  They must be in
  the column space of $G$ also.  Are your answers unique?  Do you
  notice anything about the scalars that appear in the linear
  combinations you are forming?
\end{example}

So we have three different methods to obtain a description of the
column space of a matrix as the span of a linearly independent set.
\ref{theorem:BCS} is sometimes useful since the vectors it specifies
are equal to actual columns of the matrix. \ref{theorem:BRS} and
\ref{theorem:CSRST} combine to create vectors with lots of zeros, and
strategically placed 1's near the top of the vector.  \ref{theorem:FS}
and the matrix $L$ from the extended echelon form gives us a third
method, which tends to create vectors with lots of zeros, and
strategically placed 1's near the bottom of the vector.  If we do not
care about linear independence we can also appeal to
\ref{definition:CSM} and simply express the column space as the span
of all the columns of the matrix, giving us a fourth description.


With \ref{theorem:CSRST} and \ref{definition:RSM}, we can compute
column spaces with theorems about row spaces, and we can compute row
spaces with theorems about column spaces, but in each case we must
transpose the matrix first.  At this point you may be overwhelmed by
all the possibilities for computing column and row spaces.
\ref{diagram:CSRST} is meant to help.  For both the column space and
row space, it suggests four techniques.  One is to appeal to the
definition, another yields a span of a linearly independent set, and a
third uses \ref{theorem:FS}.  A fourth suggests transposing the matrix
and the dashed line implies that then the companion set of techniques
can be applied.  This can lead to a bit of silliness, since if you
were to follow the dashed lines \textit{twice} you would transpose the
matrix twice, and by \ref{theorem:TT} would accomplish nothing
productive.

\begin{tikzpicture}
\node (CA)  at (0em, 13em) [shape=rectangle] {$\csp{A}$};
\node (CSM) at (6em, 16em) [shape=rectangle, anchor=west] {Definition CSM};
\node (BCS) at (6em, 14em) [shape=rectangle, anchor=west] {Theorem BCS};
\node (FSL) at (6em, 12em) [shape=rectangle, anchor=west] {Theorem FS, $\nsp{L}$};
\node (CSR) at (6em, 10em) [shape=rectangle, anchor=west] {Theorem CSRST, $\rsp{\transpose{A}}$};
\node (RA)  at (0em,  3em) [shape=rectangle] {$\rsp{A}$};
\node (RST) at (6em,  6em) [shape=rectangle, anchor=west] {Definition RSM, $\csp{\transpose{A}}$};
\node (FSC) at (6em,  4em) [shape=rectangle, anchor=west] {Theorem FS, $\rsp{C}$};
\node (BRS) at (6em,  2em) [shape=rectangle, anchor=west] {Theorem BRS};
\node (RSM) at (6em,  0em) [shape=rectangle, anchor=west] {Definition RSM};
\draw[->,thick] (CA) to  (CSM.west);
\draw[->,thick] (CA) to  (BCS.west);
\draw[->,thick] (CA) to  (FSL.west);
\draw[->,thick] (CA) to  (CSR.west);
\draw[->,thick] (RA) to  (RST.west);
\draw[->,thick] (RA) to  (FSC.west);
\draw[->,thick] (RA) to  (BRS.west);
\draw[->,thick] (RA) to  (RSM.west);
\draw[->,densely dashed,thick]
(RST.east) .. controls ([xshift=9em,yshift=3em]RST.east) and ([yshift=-1em]RST.west) ..  (CA.south);
\draw[->,densely dashed,thick]
(CSR.east) .. controls ([xshift=6.5em,yshift=-3em]CSR.east) and ([yshift=5em]RST.west) ..  (RA.north);
\end{tikzpicture}

Although we have many ways to describe a column space, there is one
tempting strategy to consider.

\begin{question}
  Is it possible to simply row-reduce a matrix directly and then use
  the columns of the row-reduced matrix as a set whose span equals the
  column space?
  \begin{multipleChoice}
    \choice{Yes.}
    \choice[correct]{No.}
  \end{multipleChoice}

  \begin{feedback}[correct]
    This tempting strategy will usually fail.  It is not possible to
    simply row-reduce a matrix directly and then use the columns of
    the row-reduced matrix as a set whose span equals the column
    space.  In other words, row operations \textit{do not} preserve
    column spaces (however row operations do preserve row spaces,
    \ref{theorem:REMRS}).
  \end{feedback}
\end{question}

\end{document}
